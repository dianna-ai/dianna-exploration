{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of LRP with binary MNIST\n",
    "\n",
    "**Function        : Exploration of LRP with binary MNIST**<br>\n",
    "**Author          : Team DIANNA**<br>\n",
    "**Contributor     :**<br>\n",
    "**First Built     : 2021.07.13**<br>\n",
    "**Last Update     : 2021.09.03**<br>\n",
    "**Library         : os, numpy, matplotlib, torch, captum**<br>\n",
    "**Description     : In this notebook we test XAI method LRP using trained binary MNIST model.**<br>\n",
    "**Return Values   : Relevance scores**<br>\n",
    "**Note**          : We use Captum library to perform LRP. This library works only with pytorch and it is not compitable with onnx.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pathlib\n",
    "import time as tt\n",
    "import numpy as np\n",
    "# DL framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "#import torch.onnx\n",
    "# XAI framework\n",
    "from captum.attr import LRP\n",
    "from captum.attr import visualization as viz\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to the dataset and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please specify data path\n",
    "datapath = pathlib.Path.home() / 'SURFdrive/Shared/datasets/mnist'\n",
    "# please specify model path\n",
    "model_path = pathlib.Path.cwd() / '../../model_generation/MNIST'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data (binary MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load binary MNIST from local\n",
    "# load data\n",
    "fd = np.load(datapath / 'binary-mnist.npz')\n",
    "# training set\n",
    "train_X = fd['X_train']\n",
    "train_y = fd['y_train']\n",
    "# testing set\n",
    "test_X = fd['X_test']\n",
    "test_y = fd['y_test']\n",
    "fd.close()\n",
    "\n",
    "# dimensions of data\n",
    "print(\"dimensions of mnist:\")\n",
    "print(\"dimensions or training set\", train_X.shape)\n",
    "print(\"dimensions or training set label\", train_y.shape)\n",
    "print(\"dimensions or testing set\", test_X.shape)\n",
    "print(\"dimensions or testing set label\", test_y.shape)\n",
    "# statistics of training set\n",
    "print(\"statistics of training set:\")\n",
    "print(\"Digits: 0 1\")\n",
    "print(\"labels: {}\".format(np.unique(train_y)))\n",
    "print(\"Class distribution: {}\".format(np.bincount(train_y)))\n",
    "print(\"First few labels of training set\", train_y[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data as torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch data loader\n",
    "test_X_torch = torch.from_numpy(test_X).type(torch.FloatTensor)\n",
    "test_y_torch = torch.from_numpy(test_y).type(torch.LongTensor)\n",
    "# reshape the input following the definition in pytorch (batch, channel, Height, Width)\n",
    "test_X_torch = test_X_torch.view(-1,1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model (Pytorch model trained for binary MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model first\n",
    "class MnistNet(nn.Module):\n",
    "    def __init__(self, kernels=[16, 32], dropout = 0.1, classes=2):\n",
    "        '''\n",
    "        Two layer CNN model with max pooling.\n",
    "        '''\n",
    "        super(MnistNet, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        # 1st layer\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        # 2nd layer\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(7 * 7 * kernels[-1], kernels[-1]) # pixel 28 / maxpooling 2 * 2 = 7\n",
    "        self.fc2 = nn.Linear(kernels[-1], classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# hyper-parameters\n",
    "kernels = [16, 32]\n",
    "dropout = 0.5\n",
    "classes = 2\n",
    "# create model\n",
    "model = MnistNet(kernels, dropout, classes)\n",
    "# load whole model state\n",
    "checkpoint = torch.load(model_path / 'mnistnet_training_checkpoint.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the class of the input image <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the prediction\n",
    "model.eval()\n",
    "\n",
    "# overall test accuracy\n",
    "correct = 0\n",
    "for i in range(len(test_X_torch)):\n",
    "    output = model(test_X_torch[i:i+1,:,:,:])\n",
    "    predicted = torch.max(output,1)[1]\n",
    "    correct += (predicted == test_y[i]).sum()\n",
    "\n",
    "print(\"Test accuracy:{:.3f}% \".format(float(correct*100) / float(len(test_X_torch))))\n",
    "\n",
    "# check one case\n",
    "output = model(test_X_torch[:1,:,:,:])\n",
    "predicted = torch.max(output,1)[1]\n",
    "print(\"prediction\", predicted)\n",
    "print(\"ground truth\", test_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRP-based attribution <br>\n",
    "Compute attributions using LRP and visualize them on the image. <br>\n",
    "Layer-wise relevance propagation is based on a backward propagation mechanism applied sequentially to all layers of the model. The model output score represents the initial relevance which is decomposed into values for each neuron of the underlying layers.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp = LRP(model)\n",
    "\n",
    "case = 10\n",
    "for i in range(case):\n",
    "    # note that the model is so well trained and the output can sometimes be 0 \n",
    "    # (which means the prediction is close to 1, given the output calculated as log_softmax)\n",
    "    # This will cause an error for this implementation. To avoid this, we check the other class (`target=(1-test_y_torch[i])`),\n",
    "    # which is equivalent since the results all indicate the relevance pixel for the prediction (two sides of one coin)\n",
    "    attributions_lrp = lrp.attribute(test_X_torch[i:i+1,:,:,:], target=(1-test_y_torch[i]))\n",
    "    # display attraibution map alongside with the predictand\n",
    "    _ = viz.visualize_image_attr_multiple(np.transpose(attributions_lrp[0,:,:,:].cpu().detach().numpy(), (1,2,0)),\n",
    "                                          np.transpose(test_X_torch[i,:,:,:].cpu().detach().numpy(), (1,2,0)),\n",
    "                                          [\"original_image\", \"blended_heat_map\"],\n",
    "                                          [\"all\", \"absolute_value\"],\n",
    "                                          show_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The above examples are pixel-wise decompositions for a multilayer neural network trained and tested on MNIST digits, using layer-wise relevance propagation. These heat maps show the pixels that support the classification. Pixels/super pixels with higher relevance scores indicate higher importance as the evidence for being the predicted class.<br>\n",
    "\n",
    "The results suggest that pixels \"inside\" the digits are important for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "case0 = np.transpose(test_X_torch[i,:,:,:].cpu().detach().numpy(), (1,2,0))\n",
    "attributions_lrp = lrp.attribute(test_X_torch[i:i+1,:,:,:], target=(1-test_y_torch[i]))\n",
    "attributions0 = np.transpose(attributions_lrp[0,:,:,:].cpu().detach().numpy(), (1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, constrained_layout=True, figsize=(10,6))\n",
    "character_plot = ax[0].imshow(case0[...,0])\n",
    "fig.colorbar(character_plot, ax=ax[0], shrink=0.67)\n",
    "\n",
    "# determine value limits to center colorbar on zero\n",
    "vmax = max(attributions0[...,0].max(), abs(attributions0[...,0].min()))\n",
    "vmin = -vmax\n",
    "\n",
    "attributions_plot = ax[1].imshow(attributions0[...,0], cmap='bwr_r', vmin=vmin, vmax=vmax)\n",
    "fig.colorbar(attributions_plot, ax=ax[1], shrink=0.67)\n",
    "\n",
    "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_lrp[0,:,:,:].cpu().detach().numpy(), (1,2,0)),\n",
    "                                      np.transpose(test_X_torch[i,:,:,:].cpu().detach().numpy(), (1,2,0)),\n",
    "                                      [\"original_image\", \"blended_heat_map\"],\n",
    "                                      [\"all\", \"absolute_value\"],\n",
    "                                      show_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion 2\n",
    "I wanted to check out the raw attributions numbers, visualize them myself to see whether they made more sense to me than with the Captum visualization. Unfortunately, it does not. On the one hand, the 0's make more sense than the 1's in the Captum viz, but this could just be a manufactured visualization choice in Captum as well. The non-zero pixels don't make much sense. Why are some red and some blue? No idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
