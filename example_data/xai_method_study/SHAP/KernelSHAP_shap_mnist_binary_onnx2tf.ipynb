{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of KernelSHAP with model (binary MNIST) converted from onnx to tf\n",
    "\n",
    "**Function        : Exploration of KernelSHAP with model (binary MNIST) converted from onnx to tf**<br>\n",
    "**Author          : Team DIANNA**<br>\n",
    "**Contributor     :**<br>\n",
    "**First Built     : 2021.12.09**<br>\n",
    "**Last Update     : 2021.12.09**<br>\n",
    "**Library         : os, numpy, matplotlib, tensorflow, keras, shap**<br>\n",
    "**Description     : In this notebook we test XAI method KernelSHAP using trained binary MNIST model.**<br>\n",
    "**Return Values   : Shapley scores**<br>\n",
    "**Note**          : We use shap library, which is the original implementation by the author of \"SHAP\" paper, to perform KernelSHAP.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import slic\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering\n",
      "training set shape (60000, 28, 28)\n",
      "training set label shape (60000,)\n",
      "testing set shape (10000, 28, 28)\n",
      "testing set label shape (10000,)\n",
      "After filtering\n",
      "training set shape (12665, 28, 28)\n",
      "training set label shape (12665,)\n",
      "testing set shape (2115, 28, 28)\n",
      "testing set label shape (2115,)\n"
     ]
    }
   ],
   "source": [
    "# prepare binary mnist dataset\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# check the shape of train and test sets\n",
    "print(\"Before filtering\")\n",
    "print(\"training set shape\", x_train.shape)\n",
    "print(\"training set label shape\", y_train.shape)\n",
    "print(\"testing set shape\", x_test.shape)\n",
    "print(\"testing set label shape\", y_test.shape)\n",
    "# get all the images labelled \"0\" and \"1\" (binary filtering)\n",
    "label_a = 0\n",
    "label_b = 1\n",
    "x_train_binary = x_train[(y_train == label_a) | (y_train == label_b),:]\n",
    "y_train_binary = y_train[(y_train == label_a) | (y_train == label_b)]\n",
    "x_test_binary = x_test[(y_test == label_a) | (y_test == label_b),:]\n",
    "y_test_binary = y_test[(y_test == label_a) | (y_test == label_b)]\n",
    "# check the shape of train and test sets after filtering\n",
    "print(\"After filtering\")\n",
    "print(\"training set shape\", x_train_binary.shape)\n",
    "print(\"training set label shape\", y_train_binary.shape)\n",
    "print(\"testing set shape\", x_test_binary.shape)\n",
    "print(\"testing set label shape\", y_test_binary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_binary shape: (12665, 28, 28, 1)\n",
      "12665 train samples\n",
      "2115 test samples\n"
     ]
    }
   ],
   "source": [
    "# define basic parameters of training\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 10\n",
    "\n",
    "# preprocess training and testing sets\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train_binary = x_train_binary.reshape(x_train_binary.shape[0], 1, img_rows, img_cols)\n",
    "    x_test_binary = x_test_binary.reshape(x_test_binary.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train_binary = x_train_binary.reshape(x_train_binary.shape[0], img_rows, img_cols, 1)\n",
    "    x_test_binary = x_test_binary.reshape(x_test_binary.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train_binary = x_train_binary.astype('float32')\n",
    "x_test_binary = x_test_binary.astype('float32')\n",
    "x_train_binary /= 255\n",
    "x_test_binary /= 255\n",
    "print('x_train_binary shape:', x_train_binary.shape)\n",
    "print(x_train_binary.shape[0], 'train samples')\n",
    "print(x_test_binary.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_binary = keras.utils.to_categorical(y_train_binary, num_classes)\n",
    "y_test_binary = keras.utils.to_categorical(y_test_binary, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 17:49:12.683825: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-03 17:49:12.684215: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-03 17:49:12.685337: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 12:21:46.642986: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-12-09 12:21:46.643706: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2304005000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "99/99 [==============================] - 18s 177ms/step - loss: 0.6926 - accuracy: 0.5235 - val_loss: 0.6692 - val_accuracy: 0.9480\n",
      "Epoch 2/2\n",
      "99/99 [==============================] - 14s 144ms/step - loss: 0.6649 - accuracy: 0.7539 - val_loss: 0.6392 - val_accuracy: 0.9844\n",
      "Test loss: 0.6391953229904175\n",
      "Test accuracy: 0.9843971729278564\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train_binary, y_train_binary,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test_binary, y_test_binary))\n",
    "score = model.evaluate(x_test_binary, y_test_binary, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "savedmodel_dir = 'mnist_savedmodel'\n",
    "#model.save(savedmodel_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yangliu/miniconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-01-03 17:49:37,681 - INFO - Signatures found in model: [serving_default].\n",
      "2022-01-03 17:49:37,681 - INFO - Output names: ['dense_1']\n",
      "WARNING:tensorflow:From /home/yangliu/miniconda3/lib/python3.8/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-01-03 17:49:37,818 - WARNING - From /home/yangliu/miniconda3/lib/python3.8/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-01-03 17:49:37,911 - INFO - Using tensorflow=2.4.1, onnx=1.9.0, tf2onnx=1.9.3/1190aa\n",
      "2022-01-03 17:49:37,912 - INFO - Using opset <onnx, 9>\n",
      "2022-01-03 17:49:38,144 - INFO - Computed 0 values for constant folding\n",
      "2022-01-03 17:49:38,281 - INFO - Optimizing ONNX model\n",
      "2022-01-03 17:49:38,413 - INFO - After optimization: Cast -1 (1->0), Const +1 (9->10), Identity -7 (7->0), Reshape +1 (1->2), Transpose -5 (6->1)\n",
      "2022-01-03 17:49:38,432 - INFO - \n",
      "2022-01-03 17:49:38,432 - INFO - Successfully converted TensorFlow model mnist_savedmodel to ONNX\n",
      "2022-01-03 17:49:38,432 - INFO - Model inputs: ['conv2d_input']\n",
      "2022-01-03 17:49:38,432 - INFO - Model outputs: ['dense_1']\n",
      "2022-01-03 17:49:38,433 - INFO - ONNX model is saved at mnist_savedmodel.onnx\n"
     ]
    }
   ],
   "source": [
    "# convert model to onnx format\n",
    "import onnx\n",
    "onnx_savedmodel = 'mnist_savedmodel.onnx'\n",
    "!python -m tf2onnx.convert --saved-model {savedmodel_dir} --output {onnx_savedmodel} --signature_def serving_default --tag serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "from onnx_tf.backend import prepare\n",
    "# Load saved onnx model and convert it back to keras/tf model\n",
    "onnx_model = onnx.load(\"mnist_savedmodel.onnx\")  # load onnx model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of tf model re-loaded from onnx format (not required step for the whole workflow)\n",
    "We can check the tf model converted back from onnx format. The following commands will do the job for you. But this part is not necessary for the execution/creation of model runner to perform kernel shap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv2d_input']\n",
      "-----\n",
      "['dense_1']\n",
      "-----\n",
      "{'conv2d_input': <tf.Tensor 'conv2d_input:0' shape=(None, 28, 28, 1) dtype=float32>, 'new_shape__19': <tf.Variable 'new_shape__19:0' shape=(4,) dtype=int64>, 'const_fold_opt__20': <tf.Variable 'const_fold_opt__20:0' shape=(2,) dtype=int64>, 'StatefulPartitionedCall/sequential/dense_1/MatMul/ReadVariableOp:0': <tf.Variable 'StatefulPartitionedCall/sequential/dense_1/MatMul/ReadVariableOp_tf_0_ff6b2649:0' shape=(128, 2) dtype=float32>, 'StatefulPartitionedCall/sequential/dense_1/BiasAdd/ReadVariableOp:0': <tf.Variable 'StatefulPartitionedCall/sequential/dense_1/BiasAdd/ReadVariableOp_tf_0_74b81394:0' shape=(2,) dtype=float32>, 'StatefulPartitionedCall/sequential/dense/MatMul/ReadVariableOp:0': <tf.Variable 'StatefulPartitionedCall/sequential/dense/MatMul/ReadVariableOp_tf_0_00b2a6d5:0' shape=(9216, 128) dtype=float32>, 'StatefulPartitionedCall/sequential/dense/BiasAdd/ReadVariableOp:0': <tf.Variable 'StatefulPartitionedCall/sequential/dense/BiasAdd/ReadVariableOp_tf_0_d9a96e3e:0' shape=(128,) dtype=float32>, 'StatefulPartitionedCall/sequential/conv2d_1/Conv2D/ReadVariableOp:0': <tf.Variable 'StatefulPartitionedCall/sequential/conv2d_1/Conv2D/ReadVariableOp_tf_0_d0b8befb:0' shape=(64, 32, 3, 3) dtype=float32>, 'StatefulPartitionedCall/sequential/conv2d_1/BiasAdd/ReadVariableOp:0': <tf.Variable 'StatefulPartitionedCall/sequential/conv2d_1/BiasAdd/ReadVariableOp_tf_0_988c20a3:0' shape=(64,) dtype=float32>, 'StatefulPartitionedCall/sequential/conv2d/Conv2D/ReadVariableOp:0': <tf.Variable 'StatefulPartitionedCall/sequential/conv2d/Conv2D/ReadVariableOp_tf_0_50b5b93b:0' shape=(32, 1, 3, 3) dtype=float32>, 'StatefulPartitionedCall/sequential/conv2d/BiasAdd/ReadVariableOp:0': <tf.Variable 'StatefulPartitionedCall/sequential/conv2d/BiasAdd/ReadVariableOp_tf_0_9d235d8d:0' shape=(32,) dtype=float32>, '_onnx_tf_internal_is_training': <tf.Tensor 'PlaceholderWithDefault:0' shape=() dtype=bool>, 'StatefulPartitionedCall/sequential/conv2d/BiasAdd__6:0': <tf.Tensor 'onnx_tf_prefix_StatefulPartitionedCall/sequential/conv2d/BiasAdd__6:0' shape=(None, None, None, None) dtype=float32>, 'StatefulPartitionedCall/sequential/conv2d/BiasAdd:0': <tf.Tensor 'transpose_2:0' shape=(None, 32, None, None) dtype=float32>, 'StatefulPartitionedCall/sequential/conv2d/Relu:0': <tf.Tensor 'onnx_tf_prefix_StatefulPartitionedCall/sequential/conv2d/Relu:0' shape=(None, 32, None, None) dtype=float32>, 'StatefulPartitionedCall/sequential/conv2d_1/BiasAdd:0': <tf.Tensor 'transpose_5:0' shape=(None, 64, None, None) dtype=float32>, 'StatefulPartitionedCall/sequential/conv2d_1/Relu:0': <tf.Tensor 'onnx_tf_prefix_StatefulPartitionedCall/sequential/conv2d_1/Relu:0' shape=(None, 64, None, None) dtype=float32>, 'StatefulPartitionedCall/sequential/max_pooling2d/MaxPool:0': <tf.Tensor 'transpose_7:0' shape=(None, 64, None, None) dtype=float32>, 'StatefulPartitionedCall/sequential/max_pooling2d/MaxPool__16:0': <tf.Tensor 'onnx_tf_prefix_StatefulPartitionedCall/sequential/max_pooling2d/MaxPool__16:0' shape=(None, None, None, 64) dtype=float32>, 'StatefulPartitionedCall/sequential/flatten/Reshape:0': <tf.Tensor 'onnx_tf_prefix_StatefulPartitionedCall/sequential/flatten/Reshape:0' shape=(None, None) dtype=float32>, 'StatefulPartitionedCall/sequential/dense/MatMul:0': <tf.Tensor 'onnx_tf_prefix_StatefulPartitionedCall/sequential/dense/MatMul:0' shape=(None, 128) dtype=float32>, 'StatefulPartitionedCall/sequential/dense/BiasAdd:0': <tf.Tensor 'onnx_tf_prefix_StatefulPartitionedCall/sequential/dense/BiasAdd:0' shape=(None, 128) dtype=float32>, 'StatefulPartitionedCall/sequential/dense/Relu:0': <tf.Tensor 'onnx_tf_prefix_StatefulPartitionedCall/sequential/dense/Relu:0' shape=(None, 128) dtype=float32>, 'StatefulPartitionedCall/sequential/dense_1/MatMul:0': <tf.Tensor 'onnx_tf_prefix_StatefulPartitionedCall/sequential/dense_1/MatMul:0' shape=(None, 2) dtype=float32>, 'StatefulPartitionedCall/sequential/dense_1/BiasAdd:0': <tf.Tensor 'onnx_tf_prefix_StatefulPartitionedCall/sequential/dense_1/BiasAdd:0' shape=(None, 2) dtype=float32>, 'dense_1': <tf.Tensor 'onnx_tf_prefix_StatefulPartitionedCall/sequential/dense_1/Softmax:0' shape=(None, 2) dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "# overview of loaded \n",
    "tf_model_rep = prepare(onnx_model, gen_tensor_dict=True)\n",
    "print(tf_model_rep.inputs) # Input nodes to the model\n",
    "print('-----')\n",
    "print(tf_model_rep.outputs) # Output nodes from the model\n",
    "print('-----')\n",
    "print(tf_model_rep.tensor_dict) # All nodes in the model\n",
    "# load tf model from exported graph\n",
    "#tf_model_rep.export_graph(\"mnist_model_graph\")\n",
    "#tf_model = tf.saved_model.load(\"mnist_model_graph\")\n",
    "#tf_model = tf.keras.models.load_model(\"mnist_model_graph\") # keras model loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFFElEQVR4nO3dTYokVRiG0fzpUuiBP+2gdeACnIibcAnuzL24IEUcOhJptNOBUyuFeiniaeqcaRNxb2TlkxcaPuJ8u91OQM/l6A0A/02cECVOiBInRIkTol7d+8fvLz9s/5V7Pg/Xbr8b58uw9vW6rb0892X8vTx078O1p9O292Xfp9P0fTtft7/ZT7/++J+bd3JClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVF35zlfffXldvdlNnGda1xmC+fZwKdffxtnA4/c++nIvY/PfbsOf7P1M3+EkxOixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcEHV3ZOz92zfTzZcxnGPHtsbxo2n0aVp6H51aRu3Gn/rlc5v2fTpNn/vteSbGnJxQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtTdec4/377e7r7M560zcsPPzvxKt2U28Lotfejex6Vvy1GxzrEuz/1MR5yTE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQdX9k7Iu7//y/tjGclzr6NK596KjduPahf7MDX9v4CCcnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBB1f57zzdbuMtd46Gzg+pN14Fzi+nM7rX/kLOoHvPZjnJwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihKj/GRkb737kCNDl9vRrX+i42nr9h/wKwNOR35dHODkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihKi785zvPn+/3X2aa3z6fN269pGzpEfOY/57/Qe690O/L+Paj3ByQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRd+c5b2/ebXcfZuTO64zcsvY4GziNBo7PPe99WP88znNeLk+fH17XXp77Yp4TXhZxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidE3R0Z+/SzP6abL2M41wNHn9YRoOm5x7XXvS/XX4eRr3Xty2l77mXvRsbghREnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCHq7jzn15/9vt38/PeTrz1yLvHV+cC5xHHtdR50efYj9758106ncY71tD33Y5ycECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSouyNj33zy23TzZZRmHRl7GEaIruvI2PA6uofLX9Pa1/FVeMvY17r2w/npz75+X5bv6rLve5ycECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocULU3XnOb1//PN18m5HbXum2zGRexle6fbS8+nBce51FneZg170P86Dr6wc/OnD2+NH7PstdgZk4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROi7o6MfffxL9PNHw58ndzyq/NwnpY+XZdrz9viD6ft+suw/nVc++H89E/uMp4zl+Gvdj0/zxnn5IQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTos632/O8vgzYODkhSpwQJU6IEidEiROixAlR/wBhOoo7djEveAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create segmentation to explain by segment, not every pixel\n",
    "segments_slic = slic(x_test_binary[1,:,:,:], n_segments=200, compactness=20, sigma=0)\n",
    "\n",
    "plt.imshow(segments_slic)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that depends on a binary mask representing if an image region is hidden\n",
    "def mask_image(zs, segmentation, image, background=None):\n",
    "    \n",
    "    if background is None:\n",
    "        background = image.mean((0, 1))\n",
    "        \n",
    "    # Create an empty 4D array\n",
    "    out = np.zeros((zs.shape[0], \n",
    "                    image.shape[0], \n",
    "                    image.shape[1], \n",
    "                    image.shape[2]))\n",
    "    \n",
    "    for i in range(zs.shape[0]):\n",
    "        out[i, :, :, :] = image\n",
    "        for j in range(zs.shape[1]):\n",
    "            if zs[i, j] == 0:\n",
    "                out[i][segmentation == j, :] = background\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def f(z):\n",
    "    # it is better to find a generic way to load the values\n",
    "    return prepare(onnx_model).run(\n",
    "        mask_image(z, segments_slic, x_test_binary[1,:,:,:], 0)).dense_1 # better replace \"dense_1\" with some generic entry\n",
    "        \n",
    "def fill_segmentation(values, segmentation):\n",
    "    out = np.zeros(segmentation.shape)\n",
    "    for i in range(len(values)):\n",
    "        out[segmentation == i] = values[i]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC00lEQVR4nO3YsQ3DMAwAwcjIahnBU2aE7BZmASGd4S/uSqph8yCgNTMPoOe4ewFgT5wQJU6IEidEiROinv8eX8fpKxcu9vm+127uckKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocULUmpm7dwA2XE6IEidEiROixAlR4oQocULUD0EoC8mGDyx+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "masked_images = mask_image(np.zeros((1,200)), segments_slic, x_test_binary[1,:,:,:], 0)\n",
    "\n",
    "plt.imshow(masked_images[0][:,:,0])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method BackendTFModule.__call__ of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f079c53eb20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method BackendTFModule.__call__ of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f079c53eb20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 17:50:34.192713: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-01-03 17:50:34.194014: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2304005000 Hz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b32483d2314b1697bf6a2f8ca06a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Kernel SHAP to explain the network's predictions\n",
    "explainer = shap.KernelExplainer(f, np.zeros((1,200)))\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    shap_values = explainer.shap_values(np.ones((1,200)), nsamples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = prepare(onnx_model).run(np.expand_dims(x_test_binary[1,:,:,:].copy(), axis=0)).dense_1\n",
    "top_preds = np.argsort(-predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the explanations\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,4))\n",
    "inds = top_preds[0]\n",
    "axes[0].imshow(x_test_binary[1,:,:,:])\n",
    "axes[0].axis('off')\n",
    "\n",
    "max_val = np.max([np.max(np.abs(shap_values[i][:,:-1])) for i in range(len(shap_values))])\n",
    "for i in range(2):\n",
    "    m = fill_segmentation(shap_values[inds[i]][0], segments_slic)\n",
    "    #axes[i+1].set_title(feature_names[str(inds[i])][1])\n",
    "    axes[i+1].imshow(x_test_binary[1,:,:,:], alpha=0.15)\n",
    "    im = axes[i+1].imshow(m, vmin=-max_val, vmax=max_val)\n",
    "    axes[i+1].axis('off')\n",
    "cb = fig.colorbar(im, ax=axes.ravel().tolist(), label=\"SHAP value\", orientation=\"horizontal\", aspect=60)\n",
    "cb.outline.set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7604e8ec5f09e490e10161e37a4725039efd3ab703d81b1b8a1e00d6741866c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
