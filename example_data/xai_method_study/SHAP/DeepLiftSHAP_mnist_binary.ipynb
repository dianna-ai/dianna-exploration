{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of DeepLiftSHAP with binary MNIST\n",
    "\n",
    "**Function        : Exploration of DeepLiftSHAP with binary MNIST**<br>\n",
    "**Author          : Team DIANNA**<br>\n",
    "**Contributor     :**<br>\n",
    "**First Built     : 2021.06.30**<br>\n",
    "**Last Update     : 2021.07.04**<br>\n",
    "**Library         : os, numpy, matplotlib, torch, tensorflow**<br>\n",
    "**Description     : In this notebook we test XAI method DeepLiftSHAP using trained binary MNIST model.**<br>\n",
    "**Return Values   : Shapley scores**<br>\n",
    "**Note**          : We use Captum library to perform DeepLiftSHAP. This library works only with pytorch and it is not compitable with onnx.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time as tt\n",
    "import numpy as np\n",
    "# DL framework\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "#import torch.onnx\n",
    "# XAI framework\n",
    "from captum.attr import DeepLift\n",
    "from captum.attr import DeepLiftShap\n",
    "from captum.attr import visualization as viz\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# report and monitoring with Weights & Biases\n",
    "#import wandb\n",
    "# ONNX model and runtime\n",
    "import onnx\n",
    "#import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to the dataset and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please specify data path\n",
    "datapath = '/mnt/d/NLeSC/DIANNA/data/mnist/binary-MNIST'\n",
    "# please specify model path\n",
    "model_path = '/mnt/d/NLeSC/DIANNA/codebase/dianna/example_data/model_generation/MNIST'\n",
    "# please specify output path\n",
    "output_path = '/mnt/d/NLeSC/DIANNA/codebase/dianna/example_data/xai_method_study'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data (binary MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of mnist:\n",
      "dimensions or training set (12665, 784)\n",
      "dimensions or training set label (12665,)\n",
      "dimensions or testing set (2115, 784)\n",
      "dimensions or testing set label (2115,)\n",
      "statistics of training set:\n",
      "Digits: 0 1\n",
      "labels: [0 1]\n",
      "Class distribution: [5923 6742]\n",
      "Labels of training set [0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# load binary MNIST from local\n",
    "# load data\n",
    "fd = np.load(os.path.join(datapath, 'binary-mnist.npz'))\n",
    "# training set\n",
    "train_X = fd['X_train']\n",
    "train_y = fd['y_train']\n",
    "# testing set\n",
    "test_X = fd['X_test']\n",
    "test_y = fd['y_test']\n",
    "fd.close()\n",
    "\n",
    "# dimensions of data\n",
    "print(\"dimensions of mnist:\")\n",
    "print(\"dimensions or training set\", train_X.shape)\n",
    "print(\"dimensions or training set label\", train_y.shape)\n",
    "print(\"dimensions or testing set\", test_X.shape)\n",
    "print(\"dimensions or testing set label\", test_y.shape)\n",
    "# statistics of training set\n",
    "print(\"statistics of training set:\")\n",
    "print(\"Digits: 0 1\")\n",
    "print(\"labels: {}\".format(np.unique(train_y)))\n",
    "print(\"Class distribution: {}\".format(np.bincount(train_y)))\n",
    "print(\"Labels of training set\", train_y[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch data loader\n",
    "test_X_torch = torch.from_numpy(test_X).type(torch.FloatTensor)\n",
    "test_y_torch = torch.from_numpy(test_y).type(torch.LongTensor)\n",
    "# reshape the input following the definition in pytorch (batch, channel, Height, Width)\n",
    "test_X_torch = test_X_torch.view(-1,1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model (Pytorch model trained for binary MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the model first\n",
    "class MnistNet(nn.Module):\n",
    "    def __init__(self, kernels=[16, 32], dropout = 0.1, classes=2):\n",
    "        '''\n",
    "        Two layer CNN model with max pooling.\n",
    "        '''\n",
    "        super(MnistNet, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        # 1st layer\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        # 2nd layer\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(7 * 7 * kernels[-1], kernels[-1]) # pixel 28 / maxpooling 2 * 2 = 7\n",
    "        self.fc2 = nn.Linear(kernels[-1], classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# hyper-parameters\n",
    "kernels = [16, 32]\n",
    "dropout = 0.5\n",
    "classes = 2\n",
    "# create model\n",
    "model = MnistNet(kernels, dropout, classes)\n",
    "# load whole model state\n",
    "checkpoint = torch.load(os.path.join(model_path, 'mnistnet_training_checkpoint.pt'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the class of the input image <br>\n",
    "About how to use ONNX model: https://pytorch.org/docs/stable/onnx.html <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction tensor([1])\n",
      "ground truth 1\n"
     ]
    }
   ],
   "source": [
    "# check the prediction\n",
    "model.eval()\n",
    "output = model(test_X_torch[:1,:,:,:])\n",
    "predicted = torch.max(output,1)[1]\n",
    "print(\"prediction\", predicted)\n",
    "print(\"ground truth\", test_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepLIFT-based attribution <br>\n",
    "Compute attributions using DeepLIFT and visualize them on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangliu/miniconda3/lib/python3.8/site-packages/captum/_utils/gradient.py:53: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/home/yangliu/miniconda3/lib/python3.8/site-packages/captum/attr/_core/deep_lift.py:322: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "deeplift = DeepLift(model)\n",
    "attributions_dl = deeplift.attribute(test_X_torch[:1,:,:,:],\n",
    "                                     baselines=test_X_torch[:1,:,:,:] * 0,\n",
    "                                     target=test_y_torch[0])\n",
    "_ = viz.visualize_image_attr(np.transpose(attributions_dl[0,:,:,:].cpu().detach().numpy(), (1,2,0)),\n",
    "                                      np.transpose(test_X_torch[0,:,:,:].cpu().detach().numpy(), (1,2,0)),\n",
    "                                      method='blended_heat_map', sign=\"all\", show_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute DeepLifttShap\n",
    "Compute Shapley score based on the DeepLift algorithm. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangliu/miniconda3/lib/python3.8/site-packages/captum/attr/_core/deep_lift.py:322: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n",
      "/home/yangliu/miniconda3/lib/python3.8/site-packages/captum/attr/_utils/visualization.py:44: UserWarning: Attempting to normalize by value approximately 0, visualized resultsmay be misleading. This likely means that attribution values are allclose to 0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "deeplift_shap = DeepLiftShap(model)\n",
    "\n",
    "# Defining baseline distribution of images\n",
    "rand_img_dist = torch.cat([test_X_torch[:1,:,:,:] * 0, test_X_torch[:1,:,:,:] * 1])\n",
    "\n",
    "attributions_dlsh = deeplift_shap.attribute(test_X_torch[:1,:,:,:],\n",
    "                                     baselines=rand_img_dist,\n",
    "                                     target=test_y_torch[0])\n",
    "_ = viz.visualize_image_attr(np.transpose(attributions_dlsh[0,:,:,:].cpu().detach().numpy(), (1,2,0)),\n",
    "                             np.transpose(test_X_torch[0,:,:,:].cpu().detach().numpy(), (1,2,0)),\n",
    "                             method='blended_heat_map', sign=\"all\", show_colorbar=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90e02b2587fbb2ca467fc85381f5522fddb4a9e5fbb8605712260c849ecf752b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}