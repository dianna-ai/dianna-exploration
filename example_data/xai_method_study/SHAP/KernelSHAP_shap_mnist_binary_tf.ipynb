{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of KernelSHAP with binary MNIST\n",
    "\n",
    "**Function        : Exploration of KernelSHAP with binary MNIST**<br>\n",
    "**Author          : Team DIANNA**<br>\n",
    "**Contributor     :**<br>\n",
    "**First Built     : 2021.11.24**<br>\n",
    "**Last Update     : 2022.01.03**<br>\n",
    "**Library         : os, numpy, matplotlib, tensorflow, keras, shap**<br>\n",
    "**Description     : In this notebook we test XAI method KernelSHAP using trained binary MNIST model.**<br>\n",
    "**Return Values   : Shapley scores**<br>\n",
    "**Note**          : We use shap library, which is the original implementation by the author of \"SHAP\" paper, to perform KernelSHAP.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import slic\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering\n",
      "training set shape (60000, 28, 28)\n",
      "training set label shape (60000,)\n",
      "testing set shape (10000, 28, 28)\n",
      "testing set label shape (10000,)\n",
      "After filtering\n",
      "training set shape (12665, 28, 28)\n",
      "training set label shape (12665,)\n",
      "testing set shape (2115, 28, 28)\n",
      "testing set label shape (2115,)\n"
     ]
    }
   ],
   "source": [
    "# prepare binary mnist dataset\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# check the shape of train and test sets\n",
    "print(\"Before filtering\")\n",
    "print(\"training set shape\", x_train.shape)\n",
    "print(\"training set label shape\", y_train.shape)\n",
    "print(\"testing set shape\", x_test.shape)\n",
    "print(\"testing set label shape\", y_test.shape)\n",
    "# get all the images labelled \"0\" and \"1\" (binary filtering)\n",
    "label_a = 0\n",
    "label_b = 1\n",
    "x_train_binary = x_train[(y_train == label_a) | (y_train == label_b),:]\n",
    "y_train_binary = y_train[(y_train == label_a) | (y_train == label_b)]\n",
    "x_test_binary = x_test[(y_test == label_a) | (y_test == label_b),:]\n",
    "y_test_binary = y_test[(y_test == label_a) | (y_test == label_b)]\n",
    "# check the shape of train and test sets after filtering\n",
    "print(\"After filtering\")\n",
    "print(\"training set shape\", x_train_binary.shape)\n",
    "print(\"training set label shape\", y_train_binary.shape)\n",
    "print(\"testing set shape\", x_test_binary.shape)\n",
    "print(\"testing set label shape\", y_test_binary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_binary shape: (12665, 28, 28, 1)\n",
      "12665 train samples\n",
      "2115 test samples\n"
     ]
    }
   ],
   "source": [
    "# define basic parameters of training\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 10\n",
    "\n",
    "# preprocess training and testing sets\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train_binary = x_train_binary.reshape(x_train_binary.shape[0], 1, img_rows, img_cols)\n",
    "    x_test_binary = x_test_binary.reshape(x_test_binary.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train_binary = x_train_binary.reshape(x_train_binary.shape[0], img_rows, img_cols, 1)\n",
    "    x_test_binary = x_test_binary.reshape(x_test_binary.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train_binary = x_train_binary.astype('float32')\n",
    "x_test_binary = x_test_binary.astype('float32')\n",
    "x_train_binary /= 255\n",
    "x_test_binary /= 255\n",
    "print('x_train_binary shape:', x_train_binary.shape)\n",
    "print(x_train_binary.shape[0], 'train samples')\n",
    "print(x_test_binary.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_binary = keras.utils.to_categorical(y_train_binary, num_classes)\n",
    "y_test_binary = keras.utils.to_categorical(y_test_binary, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 13:55:58.858638: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-03 13:55:58.859291: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-03 13:55:58.861100: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 13:56:21.707918: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-01-03 13:56:21.716469: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2304005000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "99/99 [==============================] - 15s 147ms/step - loss: 0.7091 - accuracy: 0.5450 - val_loss: 0.6584 - val_accuracy: 0.6496\n",
      "Epoch 2/10\n",
      "99/99 [==============================] - 18s 181ms/step - loss: 0.6486 - accuracy: 0.7371 - val_loss: 0.5987 - val_accuracy: 0.9934\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 16s 160ms/step - loss: 0.5966 - accuracy: 0.8963 - val_loss: 0.5379 - val_accuracy: 0.9953\n",
      "Epoch 4/10\n",
      "99/99 [==============================] - 15s 155ms/step - loss: 0.5401 - accuracy: 0.9519 - val_loss: 0.4792 - val_accuracy: 0.9957\n",
      "Epoch 5/10\n",
      "99/99 [==============================] - 17s 171ms/step - loss: 0.4841 - accuracy: 0.9738 - val_loss: 0.4235 - val_accuracy: 0.9957\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - 27s 270ms/step - loss: 0.4296 - accuracy: 0.9800 - val_loss: 0.3710 - val_accuracy: 0.9972\n",
      "Epoch 7/10\n",
      "99/99 [==============================] - 22s 216ms/step - loss: 0.3837 - accuracy: 0.9824 - val_loss: 0.3224 - val_accuracy: 0.9976\n",
      "Epoch 8/10\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.3363 - accuracy: 0.9871 - val_loss: 0.2780 - val_accuracy: 0.9972\n",
      "Epoch 9/10\n",
      "99/99 [==============================] - 23s 233ms/step - loss: 0.2935 - accuracy: 0.9883 - val_loss: 0.2383 - val_accuracy: 0.9967\n",
      "Epoch 10/10\n",
      "99/99 [==============================] - 19s 190ms/step - loss: 0.2549 - accuracy: 0.9891 - val_loss: 0.2036 - val_accuracy: 0.9967\n",
      "Test loss: 0.20356319844722748\n",
      "Test accuracy: 0.9966903328895569\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train_binary, y_train_binary,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test_binary, y_test_binary))\n",
    "score = model.evaluate(x_test_binary, y_test_binary, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 14:04:49.356242: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist_savedmodel/assets\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "savedmodel_dir = 'mnist_savedmodel'\n",
    "model.save(savedmodel_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFFElEQVR4nO3dTYokVRiG0fzpUuiBP+2gdeACnIibcAnuzL24IEUcOhJptNOBUyuFeiniaeqcaRNxb2TlkxcaPuJ8u91OQM/l6A0A/02cECVOiBInRIkTol7d+8fvLz9s/5V7Pg/Xbr8b58uw9vW6rb0892X8vTx078O1p9O292Xfp9P0fTtft7/ZT7/++J+bd3JClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVF35zlfffXldvdlNnGda1xmC+fZwKdffxtnA4/c++nIvY/PfbsOf7P1M3+EkxOixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcEHV3ZOz92zfTzZcxnGPHtsbxo2n0aVp6H51aRu3Gn/rlc5v2fTpNn/vteSbGnJxQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtTdec4/377e7r7M560zcsPPzvxKt2U28Lotfejex6Vvy1GxzrEuz/1MR5yTE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQdX9k7Iu7//y/tjGclzr6NK596KjduPahf7MDX9v4CCcnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBB1f57zzdbuMtd46Gzg+pN14Fzi+nM7rX/kLOoHvPZjnJwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihKj/GRkb737kCNDl9vRrX+i42nr9h/wKwNOR35dHODkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihKi785zvPn+/3X2aa3z6fN269pGzpEfOY/57/Qe690O/L+Paj3ByQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRd+c5b2/ebXcfZuTO64zcsvY4GziNBo7PPe99WP88znNeLk+fH17XXp77Yp4TXhZxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidE3R0Z+/SzP6abL2M41wNHn9YRoOm5x7XXvS/XX4eRr3Xty2l77mXvRsbghREnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCHq7jzn15/9vt38/PeTrz1yLvHV+cC5xHHtdR50efYj9758106ncY71tD33Y5ycECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSouyNj33zy23TzZZRmHRl7GEaIruvI2PA6uofLX9Pa1/FVeMvY17r2w/npz75+X5bv6rLve5ycECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocULU3XnOb1//PN18m5HbXum2zGRexle6fbS8+nBce51FneZg170P86Dr6wc/OnD2+NH7PstdgZk4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROi7o6MfffxL9PNHw58ndzyq/NwnpY+XZdrz9viD6ft+suw/nVc++H89E/uMp4zl+Gvdj0/zxnn5IQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTos632/O8vgzYODkhSpwQJU6IEidEiROixAlR/wBhOoo7djEveAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create segmentation to explain by segment, not every pixel\n",
    "segments_slic = slic(x_test_binary[1,:,:,:], n_segments=200, compactness=20, sigma=0)\n",
    "\n",
    "plt.imshow(segments_slic)\n",
    "plt.axis('off')\n",
    "\n",
    "print(segments_slic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that depends on a binary mask representing if an image region is hidden\n",
    "def mask_image(zs, segmentation, image, background=None):\n",
    "    \n",
    "    if background is None:\n",
    "        background = image.mean((0, 1))\n",
    "        \n",
    "    # Create an empty 4D array\n",
    "    out = np.zeros((zs.shape[0], \n",
    "                    image.shape[0], \n",
    "                    image.shape[1], \n",
    "                    image.shape[2]))\n",
    "    \n",
    "    for i in range(zs.shape[0]):\n",
    "        out[i, :, :, :] = image\n",
    "        for j in range(zs.shape[1]):\n",
    "            if zs[i, j] == 0:\n",
    "                out[i][segmentation == j, :] = background\n",
    "    return out\n",
    "\n",
    "# define a model runner\n",
    "def f(z):\n",
    "    return model.predict(\n",
    "        mask_image(z, segments_slic, x_test_binary[1,:,:,:], 0))\n",
    "\n",
    "def fill_segmentation(values, segmentation):\n",
    "    out = np.zeros(segmentation.shape)\n",
    "    for i in range(len(values)):\n",
    "        out[segmentation == i] = values[i]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC00lEQVR4nO3YsQ3DMAwAwcjIahnBU2aE7BZmASGd4S/uSqph8yCgNTMPoOe4ewFgT5wQJU6IEidEiROinv8eX8fpKxcu9vm+127uckKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocULUmpm7dwA2XE6IEidEiROixAlR4oQocULUD0EoC8mGDyx+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "masked_images = mask_image(np.zeros((1,200)), segments_slic, x_test_binary[1,:,:,:], 0)\n",
    "\n",
    "plt.imshow(masked_images[0][:,:,0])\n",
    "plt.axis('off')\n",
    "\n",
    "print(masked_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c48559bfb44e2095a29e8f4a1f44ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Kernel SHAP to explain the network's predictions\n",
    "explainer = shap.KernelExplainer(f, np.zeros((1,200)))\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    shap_values = explainer.shap_values(np.ones((1,200)), nsamples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(np.expand_dims(x_test_binary[1,:,:,:].copy(), axis=0))\n",
    "top_preds = np.argsort(-predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the explanations\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,4))\n",
    "inds = top_preds[0]\n",
    "axes[0].imshow(x_test_binary[1,:,:,:])\n",
    "axes[0].axis('off')\n",
    "\n",
    "max_val = np.max([np.max(np.abs(shap_values[i][:,:-1])) for i in range(len(shap_values))])\n",
    "for i in range(2):\n",
    "    m = fill_segmentation(shap_values[inds[i]][0], segments_slic)\n",
    "    #axes[i+1].set_title(feature_names[str(inds[i])][1])\n",
    "    axes[i+1].imshow(x_test_binary[1,:,:,:], alpha=0.15)\n",
    "    im = axes[i+1].imshow(m, vmin=-max_val, vmax=max_val)\n",
    "    axes[i+1].axis('off')\n",
    "cb = fig.colorbar(im, ax=axes.ravel().tolist(), label=\"SHAP value\", orientation=\"horizontal\", aspect=60)\n",
    "cb.outline.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This demonstrates that the kernel explainer in shap works with tf models and the results are reasonable."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7604e8ec5f09e490e10161e37a4725039efd3ab703d81b1b8a1e00d6741866c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
