{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Exploration of RISE with binary MNIST\n",
    "\n",
    "**Function        : Exploration of RISE with binary MNIST**<br>\n",
    "**Author          : Team DIANNA**<br>\n",
    "**Contributor     :**<br>\n",
    "**First Built     : 2021.07.27**<br>\n",
    "**Last Update     : 2021.07.27**<br>\n",
    "**Library         : os, numpy, matplotlib, torch, torchray**<br>\n",
    "**Description     : In this notebook we test XAI method RISE (Randomized Input Sampling for Explanation of Black-box Models) using trained binary MNIST model.**<br>\n",
    "**Return Values   : Shapley scores**<br>\n",
    "**Note**          : We use torchray library to perform RISE. This library works only with pytorch and it is not compitable with onnx.<br>\n",
    "Link to the project page of the original paper: http://cs-people.bu.edu/vpetsiuk/rise/ <br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time as tt\n",
    "import numpy as np\n",
    "# DL framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "# XAI framework\n",
    "from torchray.attribution.rise import rise\n",
    "from torchray.benchmark import plot_example\n",
    "from captum.attr import visualization as viz\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Path to the dataset and the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# please specify data path\n",
    "datapath = '/mnt/d/NLeSC/DIANNA/data/mnist/binary-MNIST'\n",
    "# please specify model path\n",
    "model_path = '/mnt/d/NLeSC/DIANNA/codebase/dianna/example_data/model_generation/MNIST'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data (binary MNIST)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# load binary MNIST from local\n",
    "# load data\n",
    "fd = np.load(os.path.join(datapath, 'binary-mnist.npz'))\n",
    "# training set\n",
    "train_X = fd['X_train']\n",
    "train_y = fd['y_train']\n",
    "# testing set\n",
    "test_X = fd['X_test']\n",
    "test_y = fd['y_test']\n",
    "fd.close()\n",
    "\n",
    "# dimensions of data\n",
    "print(\"dimensions of mnist:\")\n",
    "print(\"dimensions or training set\", train_X.shape)\n",
    "print(\"dimensions or training set label\", train_y.shape)\n",
    "print(\"dimensions or testing set\", test_X.shape)\n",
    "print(\"dimensions or testing set label\", test_y.shape)\n",
    "# statistics of training set\n",
    "print(\"statistics of training set:\")\n",
    "print(\"Digits: 0 1\")\n",
    "print(\"labels: {}\".format(np.unique(train_y)))\n",
    "print(\"Class distribution: {}\".format(np.bincount(train_y)))\n",
    "print(\"Labels of training set\", train_y[:20])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dimensions of mnist:\n",
      "dimensions or training set (12665, 784)\n",
      "dimensions or training set label (12665,)\n",
      "dimensions or testing set (2115, 784)\n",
      "dimensions or testing set label (2115,)\n",
      "statistics of training set:\n",
      "Digits: 0 1\n",
      "labels: [0 1]\n",
      "Class distribution: [5923 6742]\n",
      "Labels of training set [0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare data as torch tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# use pytorch data loader\n",
    "test_X_torch = torch.from_numpy(test_X).type(torch.FloatTensor)\n",
    "test_y_torch = torch.from_numpy(test_y).type(torch.LongTensor)\n",
    "# reshape the input following the definition in pytorch (batch, channel, Height, Width)\n",
    "test_X_torch = test_X_torch.view(-1,1,28,28)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load model (Pytorch model trained for binary MNIST)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# define the model first\n",
    "class MnistNet(nn.Module):\n",
    "    def __init__(self, kernels=[16, 32], dropout = 0.1, classes=2):\n",
    "        '''\n",
    "        Two layer CNN model with max pooling.\n",
    "        '''\n",
    "        super(MnistNet, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        # 1st layer\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        # 2nd layer\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(7 * 7 * kernels[-1], kernels[-1]) # pixel 28 / maxpooling 2 * 2 = 7\n",
    "        self.fc2 = nn.Linear(kernels[-1], classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# hyper-parameters\n",
    "kernels = [16, 32]\n",
    "dropout = 0.5\n",
    "classes = 2\n",
    "# create model\n",
    "model = MnistNet(kernels, dropout, classes)\n",
    "# load whole model state\n",
    "checkpoint = torch.load(os.path.join(model_path, 'mnistnet_training_checkpoint.pt'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predict the class of the input image <br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# check the prediction\n",
    "model.eval()\n",
    "\n",
    "# overall test accuracy\n",
    "correct = 0\n",
    "for i in range(len(test_X_torch)):\n",
    "    output = model(test_X_torch[i:i+1,:,:,:])\n",
    "    predicted = torch.max(output,1)[1]\n",
    "    correct += (predicted == test_y[i]).sum()\n",
    "\n",
    "print(\"Test accuracy:{:.3f}% \".format(float(correct*100) / float(len(test_X_torch))))\n",
    "\n",
    "# check one case\n",
    "output = model(test_X_torch[:1,:,:,:])\n",
    "predicted = torch.max(output,1)[1]\n",
    "print(\"prediction\", predicted)\n",
    "print(\"ground truth\", test_y[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test accuracy:99.905% \n",
      "prediction tensor([1])\n",
      "ground truth 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RISE-based attribution <br>\n",
    "Compute attributions using RISE and visualize the saliency on the image. <br>\n",
    "RISE estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs.<br>\n",
    "In other words, RISE masks random portions of the input image and passes this image through the model — the mask that damages accuracy the most is the most “important” portion.<br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# call RISE method.\n",
    "saliency = rise(model, test_X_torch[:1,:,:,:]) # pass model and input\n",
    "saliency = saliency[:, test_y_torch[0]].unsqueeze(0) # choose the target class by passing the label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "print(saliency.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 2, 28, 28])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# display attraibution map alongside with the predictand\n",
    "_ = viz.visualize_image_attr_multiple(np.transpose(saliency[0,:1,:,:].cpu().detach().numpy(), (1,2,0)),\n",
    "                                      np.transpose(test_X_torch[0,:,:,:].cpu().detach().numpy(), (1,2,0)),\n",
    "                                      [\"original_image\", \"blended_heat_map\"],\n",
    "                                      [\"all\", \"absolute_value\"],\n",
    "                                      show_colorbar=True)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 576x432 with 4 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90e02b2587fbb2ca467fc85381f5522fddb4a9e5fbb8605712260c849ecf752b"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}