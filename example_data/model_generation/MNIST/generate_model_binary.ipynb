{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Train model for binary MNIST\n",
    "**Function        : Train model in pytorch for binary MNIST **<br>\n",
    "**Author          : Team DIANNA **<br>\n",
    "**Contributor     : **<br>\n",
    "**First Built     : 2021.06.06 **<br>\n",
    "**Last Update     : 2021.06.26 **<br>\n",
    "**Library         : os, numpy, matplotlib, torch, tensorflow, wandb **<br>\n",
    "**Description     : In this notebook we train models in pytorch for binary MNIST dataset. The trained models will be used to explore the XAI methods later.**<br>\n",
    "**Return Values   : pytorch models and training status (.pt) / training report from weights & biases**<br>\n",
    "**Note**          : The best trained model is stored as ONNX. For more information about how to load an ONNX model in pytorch, visit: https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html <br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline\n",
    "import pathlib\n",
    "import time as tt\n",
    "import numpy as np\n",
    "# DL framework\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.onnx\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# report and monitoring with Weights & Biases\n",
    "import wandb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#################################################################################\n",
    "#########                     path to the dataset                        ########\n",
    "#################################################################################\n",
    "# please specify data path\n",
    "datapath = pathlib.Path.home() / 'SURFdrive/Shared/datasets/mnist'\n",
    "# please specify output path\n",
    "output_path = pathlib.Path.cwd() / '../../model_generation/MNIST'\n",
    "output_path.mkdir(exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#################################################################################\n",
    "#########              extract binary MNIST dataset locally              ########\n",
    "#################################################################################\n",
    "# load binary MNIST from local\n",
    "# load data\n",
    "fd = np.load(datapath / 'binary-mnist.npz')\n",
    "# training set\n",
    "train_X = fd['X_train']\n",
    "train_y = fd['y_train']\n",
    "# testing set\n",
    "test_X = fd['X_test']\n",
    "test_y = fd['y_test']\n",
    "fd.close()\n",
    "\n",
    "# dimensions of data\n",
    "print(\"dimensions of mnist:\")\n",
    "print(\"dimensions or training set\", train_X.shape)\n",
    "print(\"dimensions or training set label\", train_y.shape)\n",
    "print(\"dimensions or testing set\", test_X.shape)\n",
    "print(\"dimensions or testing set label\", test_y.shape)\n",
    "# statistics of training set\n",
    "print(\"statistics of training set:\")\n",
    "print(\"Digits: 0 1\")\n",
    "print(\"labels: {}\".format(np.unique(train_y)))\n",
    "print(\"Class distribution: {}\".format(np.bincount(train_y)))\n",
    "print(\"Labels of training set\", train_y[:20])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dimensions of mnist:\n",
      "dimensions or training set (12665, 784)\n",
      "dimensions or training set label (12665,)\n",
      "dimensions or testing set (2115, 784)\n",
      "dimensions or testing set label (2115,)\n",
      "statistics of training set:\n",
      "Digits: 0 1\n",
      "labels: [0 1]\n",
      "Class distribution: [5923 6742]\n",
      "Labels of training set [0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#################################################################################\n",
    "#########              set up the environment for training               ########\n",
    "#################################################################################\n",
    "print ('*******************  check the version of pytorch  *********************')\n",
    "print (\"Pytorch version {}\".format(torch.__version__))\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "# use GPU if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device to be used for computation: {}\".format(device))\n",
    "print ('*******************  login weights & biases  *********************')\n",
    "# call weights & biases service\n",
    "wandb.login()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*******************  check the version of pytorch  *********************\n",
      "Pytorch version 1.9.1\n",
      "Is CUDA available? False\n",
      "Device to be used for computation: cpu\n",
      "*******************  login weights & biases  *********************\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33megpbos\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#################################################################################\n",
    "#########                      build neural network                      ########\n",
    "#################################################################################\n",
    "class MnistNet(nn.Module):\n",
    "    def __init__(self, kernels=[16, 32], dropout = 0.1, classes=2):\n",
    "        '''\n",
    "        Two layer CNN model with max pooling.\n",
    "        '''\n",
    "        super(MnistNet, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        # 1st layer\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        # 2nd layer\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(7 * 7 * kernels[-1], kernels[-1]) # pixel 28 / maxpooling 2 * 2 = 7\n",
    "        self.fc2 = nn.Linear(kernels[-1], classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#################################################################################\n",
    "#########            configure hyper-parameters & prepare data           ########\n",
    "#################################################################################\n",
    "hyperparameters = dict(\n",
    "    epoch = 10,\n",
    "    classes = 2,\n",
    "    kernels = [16, 32],\n",
    "    batch_size = 64,\n",
    "    learning_rate = 0.001,\n",
    "    dropout = 0.5,\n",
    "    dataset = 'MNIST',\n",
    "    architecture = 'CNN'\n",
    ")\n",
    "\n",
    "# initialize weights & biases service\n",
    "#mode = 'online'\n",
    "mode = 'disabled'\n",
    "wandb.init(config=hyperparameters, project='mnist', entity='dianna-ai', mode=mode)\n",
    "config = wandb.config\n",
    "\n",
    "# use pytorch data loader\n",
    "train_X_torch = torch.from_numpy(train_X).type(torch.FloatTensor)\n",
    "train_y_torch = torch.from_numpy(train_y).type(torch.LongTensor)\n",
    "\n",
    "test_X_torch = torch.from_numpy(test_X).type(torch.FloatTensor)\n",
    "test_y_torch = torch.from_numpy(test_y).type(torch.LongTensor)\n",
    "\n",
    "# reshape the input following the definition in pytorch (batch, channel, Height, Width)\n",
    "train_X_torch = train_X_torch.view(-1,1,28,28)\n",
    "test_X_torch = test_X_torch.view(-1,1,28,28)\n",
    "\n",
    "# pytorch train and test sets\n",
    "train_set = torch.utils.data.TensorDataset(train_X_torch,train_y_torch)\n",
    "test_set = torch.utils.data.TensorDataset(test_X_torch,test_y_torch)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = config.batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = config.batch_size, shuffle = False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#################################################################################\n",
    "#########               create model and choose optimizer                ########\n",
    "#################################################################################\n",
    "model = MnistNet(config.kernels, config.dropout, config.classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss() # same as BCELoss(torch.sigmoid(x),...), but more numerically stable\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "print('Model details:\\n', model)\n",
    "print('Optimizer details:\\n',optimizer)\n",
    "\n",
    "wandb.watch(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model details:\n",
      " MnistNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=1568, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "Optimizer details:\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#################################################################################\n",
    "#########                   start the training process                   ########\n",
    "#################################################################################\n",
    "# calculate the time for the code execution\n",
    "start_time = tt.time()\n",
    "# switch model into train mode\n",
    "model.train()\n",
    "for epoch in range(config.epoch):\n",
    "    # number of correction predictions\n",
    "    correct = 0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        var_X_batch = torch.autograd.Variable(X_batch).to(device)\n",
    "        var_y_batch = torch.autograd.Variable(y_batch).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(var_X_batch).squeeze(1)\n",
    "        loss = criterion(output, var_y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Total correct predictions\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        #predicted = torch.round(torch.sigmoid(output))\n",
    "        correct += (predicted == var_y_batch).sum()\n",
    "        # log the training loss and accuracy in wandb\n",
    "        wandb.log({'train_loss': loss.item(), 'train_acc': float(correct*100) / float(config.batch_size*(batch_idx+1))})\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                  epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.* batch_idx / len(train_loader),\n",
    "                  loss.item(), float(correct*100) / float(config.batch_size*(batch_idx+1))))\n",
    "\n",
    "# save the general checkpoint\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item()\n",
    "            }, output_path / 'mnistnet_training_checkpoint.pt')\n",
    "print(\"The checkpoint of the model and training status is saved.\")\n",
    "\n",
    "print (\"--- %s minutes ---\" % ((tt.time() - start_time)/60))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch : 0 [0/12665 (0%)]\tLoss: 0.000036\t Accuracy:100.000%\n",
      "Epoch : 0 [1280/12665 (10%)]\tLoss: 0.000323\t Accuracy:99.926%\n",
      "Epoch : 0 [2560/12665 (20%)]\tLoss: 0.000006\t Accuracy:99.924%\n",
      "Epoch : 0 [3840/12665 (30%)]\tLoss: 0.000026\t Accuracy:99.923%\n",
      "Epoch : 0 [5120/12665 (40%)]\tLoss: 0.000000\t Accuracy:99.904%\n",
      "Epoch : 0 [6400/12665 (51%)]\tLoss: 0.044152\t Accuracy:99.876%\n",
      "Epoch : 0 [7680/12665 (61%)]\tLoss: 0.000000\t Accuracy:99.819%\n",
      "Epoch : 0 [8960/12665 (71%)]\tLoss: 0.009543\t Accuracy:99.801%\n",
      "Epoch : 0 [10240/12665 (81%)]\tLoss: 0.000080\t Accuracy:99.806%\n",
      "Epoch : 0 [11520/12665 (91%)]\tLoss: 0.000000\t Accuracy:99.810%\n",
      "Epoch : 1 [0/12665 (0%)]\tLoss: 0.000003\t Accuracy:100.000%\n",
      "Epoch : 1 [1280/12665 (10%)]\tLoss: 0.000098\t Accuracy:99.926%\n",
      "Epoch : 1 [2560/12665 (20%)]\tLoss: 0.000002\t Accuracy:99.809%\n",
      "Epoch : 1 [3840/12665 (30%)]\tLoss: 0.000000\t Accuracy:99.795%\n",
      "Epoch : 1 [5120/12665 (40%)]\tLoss: 0.000000\t Accuracy:99.807%\n",
      "Epoch : 1 [6400/12665 (51%)]\tLoss: 0.000003\t Accuracy:99.799%\n",
      "Epoch : 1 [7680/12665 (61%)]\tLoss: 0.000000\t Accuracy:99.780%\n",
      "Epoch : 1 [8960/12665 (71%)]\tLoss: 0.000063\t Accuracy:99.756%\n",
      "Epoch : 1 [10240/12665 (81%)]\tLoss: 0.000013\t Accuracy:99.777%\n",
      "Epoch : 1 [11520/12665 (91%)]\tLoss: 0.000010\t Accuracy:99.784%\n",
      "Epoch : 2 [0/12665 (0%)]\tLoss: 0.045726\t Accuracy:96.875%\n",
      "Epoch : 2 [1280/12665 (10%)]\tLoss: 0.000499\t Accuracy:99.777%\n",
      "Epoch : 2 [2560/12665 (20%)]\tLoss: 0.002831\t Accuracy:99.848%\n",
      "Epoch : 2 [3840/12665 (30%)]\tLoss: 0.002880\t Accuracy:99.846%\n",
      "Epoch : 2 [5120/12665 (40%)]\tLoss: 0.000003\t Accuracy:99.807%\n",
      "Epoch : 2 [6400/12665 (51%)]\tLoss: 0.000000\t Accuracy:99.830%\n",
      "Epoch : 2 [7680/12665 (61%)]\tLoss: 0.000025\t Accuracy:99.845%\n",
      "Epoch : 2 [8960/12665 (71%)]\tLoss: 0.000002\t Accuracy:99.789%\n",
      "Epoch : 2 [10240/12665 (81%)]\tLoss: 0.000007\t Accuracy:99.816%\n",
      "Epoch : 2 [11520/12665 (91%)]\tLoss: 0.000055\t Accuracy:99.819%\n",
      "Epoch : 3 [0/12665 (0%)]\tLoss: 0.000282\t Accuracy:100.000%\n",
      "Epoch : 3 [1280/12665 (10%)]\tLoss: 0.006990\t Accuracy:99.926%\n",
      "Epoch : 3 [2560/12665 (20%)]\tLoss: 0.132516\t Accuracy:99.771%\n",
      "Epoch : 3 [3840/12665 (30%)]\tLoss: 0.000000\t Accuracy:99.795%\n",
      "Epoch : 3 [5120/12665 (40%)]\tLoss: 0.000000\t Accuracy:99.788%\n",
      "Epoch : 3 [6400/12665 (51%)]\tLoss: 0.001336\t Accuracy:99.783%\n",
      "Epoch : 3 [7680/12665 (61%)]\tLoss: 0.000001\t Accuracy:99.768%\n",
      "Epoch : 3 [8960/12665 (71%)]\tLoss: 0.000105\t Accuracy:99.756%\n",
      "Epoch : 3 [10240/12665 (81%)]\tLoss: 0.000020\t Accuracy:99.748%\n",
      "Epoch : 3 [11520/12665 (91%)]\tLoss: 0.000000\t Accuracy:99.715%\n",
      "Epoch : 4 [0/12665 (0%)]\tLoss: 0.116870\t Accuracy:98.438%\n",
      "Epoch : 4 [1280/12665 (10%)]\tLoss: 0.000000\t Accuracy:99.851%\n",
      "Epoch : 4 [2560/12665 (20%)]\tLoss: 0.000001\t Accuracy:99.733%\n",
      "Epoch : 4 [3840/12665 (30%)]\tLoss: 0.000001\t Accuracy:99.795%\n",
      "Epoch : 4 [5120/12665 (40%)]\tLoss: 0.000000\t Accuracy:99.788%\n",
      "Epoch : 4 [6400/12665 (51%)]\tLoss: 0.000085\t Accuracy:99.799%\n",
      "Epoch : 4 [7680/12665 (61%)]\tLoss: 0.000000\t Accuracy:99.806%\n",
      "Epoch : 4 [8960/12665 (71%)]\tLoss: 0.000000\t Accuracy:99.767%\n",
      "Epoch : 4 [10240/12665 (81%)]\tLoss: 0.000642\t Accuracy:99.786%\n",
      "Epoch : 4 [11520/12665 (91%)]\tLoss: 0.000000\t Accuracy:99.801%\n",
      "Epoch : 5 [0/12665 (0%)]\tLoss: 0.000129\t Accuracy:100.000%\n",
      "Epoch : 5 [1280/12665 (10%)]\tLoss: 0.071393\t Accuracy:99.777%\n",
      "Epoch : 5 [2560/12665 (20%)]\tLoss: 0.000002\t Accuracy:99.809%\n",
      "Epoch : 5 [3840/12665 (30%)]\tLoss: 0.000000\t Accuracy:99.846%\n",
      "Epoch : 5 [5120/12665 (40%)]\tLoss: 0.000000\t Accuracy:99.807%\n",
      "Epoch : 5 [6400/12665 (51%)]\tLoss: 0.000128\t Accuracy:99.814%\n",
      "Epoch : 5 [7680/12665 (61%)]\tLoss: 0.000015\t Accuracy:99.819%\n",
      "Epoch : 5 [8960/12665 (71%)]\tLoss: 0.013153\t Accuracy:99.812%\n",
      "Epoch : 5 [10240/12665 (81%)]\tLoss: 0.000000\t Accuracy:99.835%\n",
      "Epoch : 5 [11520/12665 (91%)]\tLoss: 0.000000\t Accuracy:99.853%\n",
      "Epoch : 6 [0/12665 (0%)]\tLoss: 0.002107\t Accuracy:100.000%\n",
      "Epoch : 6 [1280/12665 (10%)]\tLoss: 0.000000\t Accuracy:100.000%\n",
      "Epoch : 6 [2560/12665 (20%)]\tLoss: 0.000000\t Accuracy:99.924%\n",
      "Epoch : 6 [3840/12665 (30%)]\tLoss: 0.000000\t Accuracy:99.923%\n",
      "Epoch : 6 [5120/12665 (40%)]\tLoss: 0.000405\t Accuracy:99.904%\n",
      "Epoch : 6 [6400/12665 (51%)]\tLoss: 0.000260\t Accuracy:99.892%\n",
      "Epoch : 6 [7680/12665 (61%)]\tLoss: 0.000000\t Accuracy:99.897%\n",
      "Epoch : 6 [8960/12665 (71%)]\tLoss: 0.016764\t Accuracy:99.845%\n",
      "Epoch : 6 [10240/12665 (81%)]\tLoss: 0.000000\t Accuracy:99.835%\n",
      "Epoch : 6 [11520/12665 (91%)]\tLoss: 0.000960\t Accuracy:99.836%\n",
      "Epoch : 7 [0/12665 (0%)]\tLoss: 0.000013\t Accuracy:100.000%\n",
      "Epoch : 7 [1280/12665 (10%)]\tLoss: 0.083764\t Accuracy:99.554%\n",
      "Epoch : 7 [2560/12665 (20%)]\tLoss: 0.000000\t Accuracy:99.543%\n",
      "Epoch : 7 [3840/12665 (30%)]\tLoss: 0.000000\t Accuracy:99.667%\n",
      "Epoch : 7 [5120/12665 (40%)]\tLoss: 0.000000\t Accuracy:99.691%\n",
      "Epoch : 7 [6400/12665 (51%)]\tLoss: 0.143098\t Accuracy:99.722%\n",
      "Epoch : 7 [7680/12665 (61%)]\tLoss: 0.000000\t Accuracy:99.755%\n",
      "Epoch : 7 [8960/12665 (71%)]\tLoss: 0.000000\t Accuracy:99.789%\n",
      "Epoch : 7 [10240/12665 (81%)]\tLoss: 0.000000\t Accuracy:99.816%\n",
      "Epoch : 7 [11520/12665 (91%)]\tLoss: 0.000000\t Accuracy:99.836%\n",
      "Epoch : 8 [0/12665 (0%)]\tLoss: 0.121065\t Accuracy:98.438%\n",
      "Epoch : 8 [1280/12665 (10%)]\tLoss: 0.001796\t Accuracy:99.702%\n",
      "Epoch : 8 [2560/12665 (20%)]\tLoss: 0.000176\t Accuracy:99.771%\n",
      "Epoch : 8 [3840/12665 (30%)]\tLoss: 0.000000\t Accuracy:99.795%\n",
      "Epoch : 8 [5120/12665 (40%)]\tLoss: 0.000000\t Accuracy:99.826%\n",
      "Epoch : 8 [6400/12665 (51%)]\tLoss: 0.172264\t Accuracy:99.845%\n",
      "Epoch : 8 [7680/12665 (61%)]\tLoss: 0.000000\t Accuracy:99.858%\n",
      "Epoch : 8 [8960/12665 (71%)]\tLoss: 0.000000\t Accuracy:99.867%\n",
      "Epoch : 8 [10240/12665 (81%)]\tLoss: 0.000000\t Accuracy:99.884%\n",
      "Epoch : 8 [11520/12665 (91%)]\tLoss: 0.000000\t Accuracy:99.888%\n",
      "Epoch : 9 [0/12665 (0%)]\tLoss: 0.000000\t Accuracy:100.000%\n",
      "Epoch : 9 [1280/12665 (10%)]\tLoss: 0.000463\t Accuracy:99.926%\n",
      "Epoch : 9 [2560/12665 (20%)]\tLoss: 0.000000\t Accuracy:99.886%\n",
      "Epoch : 9 [3840/12665 (30%)]\tLoss: 0.000000\t Accuracy:99.923%\n",
      "Epoch : 9 [5120/12665 (40%)]\tLoss: 0.000000\t Accuracy:99.904%\n",
      "Epoch : 9 [6400/12665 (51%)]\tLoss: 0.000121\t Accuracy:99.907%\n",
      "Epoch : 9 [7680/12665 (61%)]\tLoss: 0.000001\t Accuracy:99.910%\n",
      "Epoch : 9 [8960/12665 (71%)]\tLoss: 0.000199\t Accuracy:99.889%\n",
      "Epoch : 9 [10240/12665 (81%)]\tLoss: 0.000000\t Accuracy:99.864%\n",
      "Epoch : 9 [11520/12665 (91%)]\tLoss: 0.000000\t Accuracy:99.862%\n",
      "The checkpoint of the model and training status is saved.\n",
      "--- 0.8613413174947103 minutes ---\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#################################################################################\n",
    "#########                  start the evaluation process                  ########\n",
    "#################################################################################\n",
    "# switch model into evaluation mode\n",
    "model.eval()\n",
    "correct = 0\n",
    "for batch_idx, (test_imgs, test_labels) in enumerate(test_loader):\n",
    "    test_imgs = torch.autograd.Variable(test_imgs).float()\n",
    "    output = model(test_imgs)\n",
    "    predicted = torch.max(output,1)[1]\n",
    "    correct += (predicted == test_labels).sum()\n",
    "    wandb.log({'test_acc': float(correct*100) / float(config.batch_size*(batch_idx+1))})\n",
    "    if batch_idx % 5 == 0:\n",
    "        print(\"Test accuracy:{:.3f}% \".format(float(correct*100) / float(config.batch_size*(batch_idx+1))))\n",
    "    \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test accuracy:100.000% \n",
      "Test accuracy:100.000% \n",
      "Test accuracy:100.000% \n",
      "Test accuracy:100.000% \n",
      "Test accuracy:100.000% \n",
      "Test accuracy:100.000% \n",
      "Test accuracy:100.000% \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#################################################################################\n",
    "#########                  check the evaluation results                  ########\n",
    "#################################################################################\n",
    "print(\"Predictions:\", predicted[:3].detach().numpy())\n",
    "print(\"Ground Truth:\", test_labels[:3])\n",
    "evaluate_fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(4, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(3):\n",
    "    a = axes[i]\n",
    "    a.imshow(test_imgs[i,0,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "    a.set_title(test_labels[i], fontsize=8)\n",
    "    a.axis('off')\n",
    "    \n",
    "evaluate_fig.tight_layout()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predictions: [1 0 1]\n",
      "Ground Truth: tensor([1, 0, 1])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAABsCAYAAACxSq3vAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKUklEQVR4nO3dW2hU2xkH8P83GTU1MyaNCTpHDUkbL6PkFFTUg1GPtiYi7Snig2iV1AteSlulPrSC4K1YqIKgD4cD2ocTS0XtS6VqUrwU9YjSKjFSc4SiRiuKtUmMxViTrD7MHDtaJ85lr7X25f+DwcnMntmf397zzfrW7NkjSikQEekQsh0AEfkXCwwRacMCQ0TasMAQkTYsMESkDQsMEWljrcCISKWIzNO8DhGRxuT1VSJyR0QOp9y/UUSm6YzBLSzk+wMROSsiX4jId5K3BSbfAPdxwO4IphKAtuSLSAjAtwFcTd70BwDz31rscwA/0hWDy1TCbL5/AWArgLrkv0Cw8g1wH0fY1ooBrAUwU0Q+AvBnJDZEP4BVyft/A+BfAKoAfB9AIRLJegngT0qp3SLycwCfJG/7oVKqXURaALQCuAkglnwMlFL/FJFIagBKqY7kO60o/x9xaDTfAD4EsFEppUSkW0SiAcs3wH0cUEpZuQD4GMAvAdQA+Cx5WxzAZ0hU/utIjLB+AGAjgDVIJBgABMBIAE3Jv2sBfJq8/hRAUfL6SQDDU9ZZCeDwW3EcAfCBrTz4Nd8ALqSs+zCAiiDl20bOk3+7ah93wyRvHMDHInIewKcAhiVv/5tSqh/APwCUADgK4EMR+S2ABUgk8kZy2b8AqE5e/1Ip9e/kdclg/QIgCO+mXzGV776U68MAdKYsE6R8AwHex222SK8AFAD4EkCzUuonACAigwCMwpsJEQCvlFI/E5HBAC4B+B6AbyXvnwrg78nr/SmPu43ERno6QBylAB7n9T/xBtP5vpFsDW4AGKaUepZcJij5BriPWy0wNwH8CsA3kNgZzyOR8N8BaH7H8p+IyI8BDEViCPhIRM6JyBcA/gOg4R2P+SOAjwD8VUS+i8TE4zdF5PdKqcUi8nUAD5PvIn5nNN8Afo3E3MDXAGwDgIDlG+A+Dkn2aL4kIgLgc6XUijT3/xTAFaXUFbOR+RPzbZ7bc+7rAkNEdrlhkpeIfIoFhoi0YYEhIm0G/BRJRDhB4wClVCbHKgBgzp2Sac6Zb2ekyzdHMESkDQsMEWnDAkNE2rDAEJE2LDBEpA0LDBFpwwJDRNqwwBCRNiwwRKQNCwyRR1y5ciWj01TW1dUhHA4jFLL/8h7wdA08jNoZ/KqAeX77qkBhYSFevHiR8fIrV65EW1sbWlpasnpcrvhVASIPKyoqymr5LVu2oL6+HtFoVFNEmfF0gYlGo5g4cSJmzJiR8VnOb9++bTtsXxs6dGjWZ55vampCeXm57dBdLdt2Z9y4cdi+fTsqKio0RZSh9/zsgnLz5fz58yoXpuPM5sVmO6f5Xnbt2pXTNnF6u/gt37FYzBV5zTbfnp6DGSj2gSROY2qOCtAcTK7bBHB2u2Sac6/ku6KiAvfu3cvpsSb293T59nSLRObV1taiubkZ3d3d6UYEOUv3Ltja2orly5ejoKDAof+Fd2zatAlKqZyLi20cwRjgpxFMvkUkH/F4HG1tbRkt65cRjBP55giGKAPxeNx2CJQlzxWYxYsXOzIcp/9XXFyMuXPnYvXq1bhw4YLjLVC+Ro0aZXX9lD3PtUheGTKm8kqLtHXrVuzatcvW6t+rq6sLJSUlGS3LFul/bLZILDAGeKXAPHz4ELFYzNbqM5LptmOBSZgyZQquXbvmUDTpcQ6GXguFQohEIigrK8PevXtftz9uLy4AUFpaiiFDhtgOw/VEBCJipLgMhAUmgCKRCObPn4/Nmzdj8+bNtsPJyqpVqzBhwgTjo1DKDQtMAEUiEcyePRvr1q2zHUrWli1bhrFjx9oOgzLEORgD3DAHs2/fPmzatEnHU1sxcuRIPH78OO39QZ+Dccs+zhFMQPipuABAQ0OD7RAoAyww5EmLFi2yHQJlgAXGxw4dOuSKA+R0qK6uth2CNm45sNEJnIMxwNYcjB920IEMtB29PAfjp32cIxgi0oYFxmfWr1/vm+E1eR9bJANMtkhBKixskdJzyz7OEQwRacMCQ0TasMAQkTYsMGTEpEmTEIvFMGjQIITDYVRXV6O+vt52WKQZJ3kN4CTvmzk/cuQIlixZ4uhzvs1rk7xObze37OMcwZBxThSX48ePOxCJP7npe2dh2wGYEgqFXPvu7ieFhYXo6+tDX18fAH0jqqtXr2p5Xi9z4zlyAlNgWFzMePnypZH13L9/38h6KD9skciTLl68aDsEykBgRjCkTyQSQW9vL169eqV9XW5sAyg9jmAob8+fP0dPT8/reZd3iUajOHv2LFvVgGGBISNisRjmzp1rOwwyjC0SOWrmzJl48uQJ2tvb0dPT8/r2aDRqMSp3Gjx4MMrKynydm8AcaGezdw/qgXapOXcqLj/98JrXD65LxQPtyPPcVDzdZuHChbZDeCdPtEiVlZVYunQppk2bZjsUVyopKcGZM2cwefJk26G8wemC4ObfzbbF7Z+qeaLA3Llzx3YIrtbR0WE7BCNaW1tth0BZ8sQcjBe/4JhK9xxMUFqHsrIyPH36NKNlgzIH45YRTLp8e2IEQ/bEYjE8evTonfcVFxdjzJgxiEQiWLNmDVavXq0lBre8iJxQVFSEeDyOyspK26EYwQJDA3rx4kXa+zo7O80F4hNtbW0YPXq07TCMYYtkgJdbpHA4/M4jdEUE/f392tb7lf7+fhQUFGT9OLe2SH76aDoVWyTKSW9vr9X1Hzt2zOr6KT88DoZcrampyXYIlAe2SAZ4uUWyrbS0NKeP4dkimcUWiTzFLS8cJwwfPhzLli1DXV2d7VCMY4Eh12lvb7cdgqNOnz6NqVOnOv68R48edfw5ncYWyQC2SNmpqqrC3bt383oON7VITm+fPXv24ObNm2hubk57jJJp6fLNAmMAC0xmnNxGfi4wbmwf+W1qIjIuEAVmzpw5tkOg9xg/frztEEgDX0/yunEoSdwuQRKIEQwR2cEC4wMnTpywHULGnj17ZjsET3PrmevSYYHxAS8dTr9t2zbbIXiSiEBEcOrUKduhZIUFxgcuX75sO4SM8RcZg4UFxgdaWlpQU1ODhoYG26G8obOzE42NjZg1axaqqqpQVlaG69ev2w7LuOnTp+f1eE+fd0cplfYCQLnhkivbcafEP2Cencz53bt3c86XE2znOtucm4hlxIgRaufOnaqtrS2nnG7YsMF6PnPNN0cwPrNjxw7bIdBburu7cfLkSezevTurx128eBH79+/HpUuXNEWmn6+/KuCW4y2UwR9eC4fDqK6uRjweR3l5OVasWIHa2tp8njKtjo4OHDhwAK2trXjw4AFu3bqFrq4uLevKVqY5N7WPh0IhhEIh1NbW4ty5c+9dvrGxEWvXrkV/fz96e3uNnD0wH+nyzQJjgMkCQwluKzB+ly7fbJGISBtPFJgFCxZktNzBgwcxb9481NTU+PoHxYm8whMtktexRTKPLZJZbJGIyDgWGCLShgWGiLRhgSEibVhgiEgbFhgi0mbAj6mJiPLBEQwRacMCQ0TasMAQkTYsMESkDQsMEWnDAkNE2vwXMr9A2ARwa30AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 288x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#################################################################################\n",
    "#########                     save the model as ONNX                     ########\n",
    "#################################################################################\n",
    "# store as ONNX\n",
    "# example input\n",
    "x = next(iter(train_loader))[0]\n",
    "torch.onnx.export(model, x, output_path / 'mnist_model.onnx', opset_version=11,\n",
    "                  export_params=True, input_names=['input'], output_names=['output'],\n",
    "                  dynamic_axes={'input': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90e02b2587fbb2ca467fc85381f5522fddb4a9e5fbb8605712260c849ecf752b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}