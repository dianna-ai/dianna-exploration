{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model for binary MNIST\n",
    "**Function        : Train model in pytorch for binary MNIST **<br>\n",
    "**Author          : Team DIANNA **<br>\n",
    "**Contributor     : **<br>\n",
    "**First Built     : 2021.06.06 **<br>\n",
    "**Last Update     : 2021.06.26 **<br>\n",
    "**Library         : os, numpy, matplotlib, torch, tensorflow, wandb **<br>\n",
    "**Description     : In this notebook we train models in pytorch for binary MNIST dataset. The trained models will be used to explore the XAI methods later.**<br>\n",
    "**Return Values   : pytorch models and training status (.pt) / training report from weights & biases**<br>\n",
    "**Note**          : The best trained model is stored as ONNX. For more information about how to load an ONNX model in pytorch, visit: https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pathlib\n",
    "import time as tt\n",
    "import numpy as np\n",
    "# DL framework\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.onnx\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# report and monitoring with Weights & Biases\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########                     path to the dataset                        ########\n",
    "#################################################################################\n",
    "# please specify data path\n",
    "datapath = pathlib.Path.home() / 'SURFdrive/Shared/datasets/mnist'\n",
    "# please specify output path\n",
    "output_path = pathlib.Path.cwd() / '../../model_generation/MNIST'\n",
    "output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########              extract binary MNIST dataset locally              ########\n",
    "#################################################################################\n",
    "# load binary MNIST from local\n",
    "# load data\n",
    "fd = np.load(datapath / 'binary-mnist.npz')\n",
    "# training set\n",
    "train_X = fd['X_train']\n",
    "train_y = fd['y_train']\n",
    "# testing set\n",
    "test_X = fd['X_test']\n",
    "test_y = fd['y_test']\n",
    "fd.close()\n",
    "\n",
    "# dimensions of data\n",
    "print(\"dimensions of mnist:\")\n",
    "print(\"dimensions or training set\", train_X.shape)\n",
    "print(\"dimensions or training set label\", train_y.shape)\n",
    "print(\"dimensions or testing set\", test_X.shape)\n",
    "print(\"dimensions or testing set label\", test_y.shape)\n",
    "# statistics of training set\n",
    "print(\"statistics of training set:\")\n",
    "print(\"Digits: 0 1\")\n",
    "print(\"labels: {}\".format(np.unique(train_y)))\n",
    "print(\"Class distribution: {}\".format(np.bincount(train_y)))\n",
    "print(\"Labels of training set\", train_y[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########              set up the environment for training               ########\n",
    "#################################################################################\n",
    "print ('*******************  check the version of pytorch  *********************')\n",
    "print (\"Pytorch version {}\".format(torch.__version__))\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "# use GPU if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device to be used for computation: {}\".format(device))\n",
    "print ('*******************  login weights & biases  *********************')\n",
    "# call weights & biases service\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########                      build neural network                      ########\n",
    "#################################################################################\n",
    "class MnistNet(nn.Module):\n",
    "    def __init__(self, kernels=[16, 32], dropout = 0.1, classes=2):\n",
    "        '''\n",
    "        Two layer CNN model with max pooling.\n",
    "        '''\n",
    "        super(MnistNet, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        # 1st layer\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        # 2nd layer\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(7 * 7 * kernels[-1], kernels[-1]) # pixel 28 / maxpooling 2 * 2 = 7\n",
    "        self.fc2 = nn.Linear(kernels[-1], classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########            configure hyper-parameters & prepare data           ########\n",
    "#################################################################################\n",
    "hyperparameters = dict(\n",
    "    epoch = 10,\n",
    "    classes = 2,\n",
    "    kernels = [16, 32],\n",
    "    batch_size = 64,\n",
    "    learning_rate = 0.001,\n",
    "    dropout = 0.5,\n",
    "    dataset = 'MNIST',\n",
    "    architecture = 'CNN'\n",
    ")\n",
    "\n",
    "# initialize weights & biases service\n",
    "#mode = 'online'\n",
    "mode = 'disabled'\n",
    "wandb.init(config=hyperparameters, project='mnist', entity='dianna-ai', mode=mode)\n",
    "config = wandb.config\n",
    "\n",
    "# use pytorch data loader\n",
    "train_X_torch = torch.from_numpy(train_X).type(torch.FloatTensor)\n",
    "train_y_torch = torch.from_numpy(train_y).type(torch.LongTensor)\n",
    "\n",
    "test_X_torch = torch.from_numpy(test_X).type(torch.FloatTensor)\n",
    "test_y_torch = torch.from_numpy(test_y).type(torch.LongTensor)\n",
    "\n",
    "# reshape the input following the definition in pytorch (batch, channel, Height, Width)\n",
    "train_X_torch = train_X_torch.view(-1,1,28,28)\n",
    "test_X_torch = test_X_torch.view(-1,1,28,28)\n",
    "\n",
    "# pytorch train and test sets\n",
    "train_set = torch.utils.data.TensorDataset(train_X_torch,train_y_torch)\n",
    "test_set = torch.utils.data.TensorDataset(test_X_torch,test_y_torch)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = config.batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = config.batch_size, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########               create model and choose optimizer                ########\n",
    "#################################################################################\n",
    "model = MnistNet(config.kernels, config.dropout, config.classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss() # same as BCELoss(torch.sigmoid(x),...), but more numerically stable\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "print('Model details:\\n', model)\n",
    "print('Optimizer details:\\n',optimizer)\n",
    "\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########                   start the training process                   ########\n",
    "#################################################################################\n",
    "# calculate the time for the code execution\n",
    "start_time = tt.time()\n",
    "# switch model into train mode\n",
    "model.train()\n",
    "for epoch in range(config.epoch):\n",
    "    # number of correction predictions\n",
    "    correct = 0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        var_X_batch = torch.autograd.Variable(X_batch).to(device)\n",
    "        var_y_batch = torch.autograd.Variable(y_batch).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(var_X_batch).squeeze(1)\n",
    "        loss = criterion(output, var_y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Total correct predictions\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        #predicted = torch.round(torch.sigmoid(output))\n",
    "        correct += (predicted == var_y_batch).sum()\n",
    "        # log the training loss and accuracy in wandb\n",
    "        wandb.log({'train_loss': loss.item(), 'train_acc': float(correct*100) / float(config.batch_size*(batch_idx+1))})\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                  epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.* batch_idx / len(train_loader),\n",
    "                  loss.item(), float(correct*100) / float(config.batch_size*(batch_idx+1))))\n",
    "\n",
    "# save the general checkpoint\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item()\n",
    "            }, output_path / 'mnistnet_training_checkpoint.pt')\n",
    "print(\"The checkpoint of the model and training status is saved.\")\n",
    "\n",
    "print (\"--- %s minutes ---\" % ((tt.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########                  start the evaluation process                  ########\n",
    "#################################################################################\n",
    "# switch model into evaluation mode\n",
    "model.eval()\n",
    "correct = 0\n",
    "for batch_idx, (test_imgs, test_labels) in enumerate(test_loader):\n",
    "    test_imgs = torch.autograd.Variable(test_imgs).float()\n",
    "    output = model(test_imgs)\n",
    "    predicted = torch.max(output,1)[1]\n",
    "    correct += (predicted == test_labels).sum()\n",
    "    wandb.log({'test_acc': float(correct*100) / float(config.batch_size*(batch_idx+1))})\n",
    "    if batch_idx % 5 == 0:\n",
    "        print(\"Test accuracy:{:.3f}% \".format(float(correct*100) / float(config.batch_size*(batch_idx+1))))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########                  check the evaluation results                  ########\n",
    "#################################################################################\n",
    "print(\"Predictions:\", predicted[:3].detach().numpy())\n",
    "print(\"Ground Truth:\", test_labels[:3])\n",
    "evaluate_fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(4, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(3):\n",
    "    a = axes[i]\n",
    "    a.imshow(test_imgs[i,0,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "    a.set_title(test_labels[i], fontsize=8)\n",
    "    a.axis('off')\n",
    "    \n",
    "evaluate_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########                     save the model as ONNX                     ########\n",
    "#################################################################################\n",
    "# store as ONNX\n",
    "# example input\n",
    "x = next(iter(train_loader))[0]\n",
    "torch.onnx.export(model, x, output_path / 'mnist_model.onnx', opset_version=11,\n",
    "                  export_params=True, input_names=['input'], output_names=['output'],\n",
    "                  dynamic_axes={'input': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
