{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd090e02b2587fbb2ca467fc85381f5522fddb4a9e5fbb8605712260c849ecf752b",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Train model for MNIST\n",
    "**Function        : Train model in pytorch for MNIST **<br>\n",
    "**Author          : Team DIANNA **<br>\n",
    "**Contributor     : **<br>\n",
    "**First Built     : 2021.06.06 **<br>\n",
    "**Last Update     : 2021.06.06 **<br>\n",
    "**Library         : os, numpy, matplotlib, torch, tensorflow, wandb **<br>\n",
    "**Description     : In this notebook we train models in pytorch for (complete) MNIST dataset. The trained models will be used to explore the XAI methods later.**<br>\n",
    "**Return Values   : pytorch models and training status (.pt) / training report from weights & biases**<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time as tt\n",
    "import numpy as np\n",
    "# DL framework\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "#import torchvision\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# report and monitoring with Weights & Biases\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########                     path to the dataset                        ########\n",
    "#################################################################################\n",
    "# please specify data path\n",
    "datapath = '/mnt/d/NLeSC/DIANNA/data/mnist'\n",
    "# please specify output path\n",
    "output_path = '/mnt/d/NLeSC/DIANNA/codebase/dianna/train_models/mnist'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########                 extract MNIST dataset locally                  ########\n",
    "#################################################################################\n",
    "# pytorch will search its MNIST folder for the data, otherwise it will start downloading\n",
    "# we can fool this downloader by placing our local mnist dataset in its path (\"/path/MNIST/raw\")\n",
    "train_data_torch = torchvision.datasets.MNIST(datapath, train=True, download=True,\n",
    "                                              transform=torchvision.transforms.Compose([\n",
    "                                              torchvision.transforms.ToTensor()]))\n",
    "test_data_torch = torchvision.datasets.MNIST(datapath, train=False, download=True,\n",
    "                                             transform=torchvision.transforms.Compose([\n",
    "                                             torchvision.transforms.ToTensor()]))\n",
    "# batch size\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "# MNIST dataset is handled by dataloader in pytorch\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data_torch,\n",
    "                                                batch_size=batch_size_train, shuffle=False)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data_torch,\n",
    "                                               batch_size=batch_size_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([64, 1, 28, 28])\ntorch.Size([64])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Digit 5')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 251.565 263.63625\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 263.63625 \nL 251.565 263.63625 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 239.758125 \nL 244.365 239.758125 \nL 244.365 22.318125 \nL 26.925 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p602d44e8f9)\">\n    <image height=\"218\" id=\"image96e2f6ea58\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-21.758125\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m4d920fa341\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m4d920fa341\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(27.626607 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m4d920fa341\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(66.455179 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m4d920fa341\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(102.1025 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m4d920fa341\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m4d920fa341\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(179.759643 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m4d920fa341\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m4716d3bf00\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4716d3bf00\" y=\"26.200982\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 30.000201)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4716d3bf00\" y=\"65.029554\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 68.828772)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4716d3bf00\" y=\"103.858125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 107.657344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4716d3bf00\" y=\"142.686696\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 146.485915)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4716d3bf00\" y=\"181.515268\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 185.314487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m4716d3bf00\" y=\"220.343839\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 224.143058)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 239.758125 \nL 26.925 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 239.758125 \nL 244.365 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 239.758125 \nL 244.365 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 22.318125 \nL 244.365 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_13\">\n    <!-- Digit 5 -->\n    <defs>\n     <path d=\"M 19.671875 64.796875 \nL 19.671875 8.109375 \nL 31.59375 8.109375 \nQ 46.6875 8.109375 53.6875 14.9375 \nQ 60.6875 21.78125 60.6875 36.53125 \nQ 60.6875 51.171875 53.6875 57.984375 \nQ 46.6875 64.796875 31.59375 64.796875 \nz\nM 9.8125 72.90625 \nL 30.078125 72.90625 \nQ 51.265625 72.90625 61.171875 64.09375 \nQ 71.09375 55.28125 71.09375 36.53125 \nQ 71.09375 17.671875 61.125 8.828125 \nQ 51.171875 0 30.078125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-68\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n     <path id=\"DejaVuSans-32\"/>\n    </defs>\n    <g transform=\"translate(115.805625 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-68\"/>\n     <use x=\"77.001953\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"104.785156\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"168.261719\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"196.044922\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"235.253906\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"267.041016\" xlink:href=\"#DejaVuSans-53\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p602d44e8f9\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "#################################################################################\n",
    "#########                 precheck data from data loader                 ########\n",
    "#################################################################################\n",
    "train_X_y = enumerate(train_data_loader)\n",
    "batch_idx, (train_X_torch, train_y_torch) = next(train_X_y)\n",
    "\n",
    "# check the data\n",
    "print(train_X_torch.shape)\n",
    "print(train_y_torch.shape)\n",
    "\n",
    "plt.imshow(train_X_torch[0,0,:,:], cmap='gray')\n",
    "plt.title(\"Digit {}\".format(train_y_torch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "*******************  check the version of pytorch  *********************\n",
      "Pytorch version 1.8.1\n",
      "Is CUDA available? False\n",
      "*******************  login weights & biases  *********************\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgit-yang\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#################################################################################\n",
    "#########              set up the environment for training               ########\n",
    "#################################################################################\n",
    "print ('*******************  check the version of pytorch  *********************')\n",
    "print (\"Pytorch version {}\".format(torch.__version__))\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "# use GPU if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print ('*******************  login weights & biases  *********************')\n",
    "# call weights & biases service\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#########                      build neural network                      ########\n",
    "#################################################################################\n",
    "class MnistNet(nn.Module):\n",
    "    def __init__(self, kernels=[16, 32], dropout = 0.1, classes=2):\n",
    "        '''\n",
    "        Two layer CNN model with max pooling.\n",
    "        '''\n",
    "        super(MnistNet, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        # 1st layer\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        # 2nd layer\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(7 * 7 * kernels[-1], kernels[-1]) # pixel 28 / maxpooling 2 * 2 = 7\n",
    "        self.fc2 = nn.Linear(kernels[-1], classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">splendid-microwave-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/dianna-ai/mnist\" target=\"_blank\">https://wandb.ai/dianna-ai/mnist</a><br/>\n                Run page: <a href=\"https://wandb.ai/dianna-ai/mnist/runs/1pjapons\" target=\"_blank\">https://wandb.ai/dianna-ai/mnist/runs/1pjapons</a><br/>\n                Run data is saved locally in <code>/mnt/d/NLeSC/DIANNA/codebase/dianna/train_models/mnist/wandb/run-20210606_143156-1pjapons</code><br/><br/>\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "#################################################################################\n",
    "#########            configure hyper-parameters & prepare data           ########\n",
    "#################################################################################\n",
    "hyperparameters = dict(\n",
    "    epoch = 10,\n",
    "    classes = 10,\n",
    "    kernels = [16, 32],\n",
    "    batch_size = batch_size_train,\n",
    "    learning_rate = 0.001,\n",
    "    dropout = 0.2,\n",
    "    dataset = 'MNIST',\n",
    "    architecture = 'CNN'\n",
    ")\n",
    "\n",
    "# initialize weights & biases service\n",
    "#os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "wandb.init(config=hyperparameters, project='mnist standard', entity='dianna-ai')\n",
    "config = wandb.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model details:\n MnistNet(\n  (layer1): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n  )\n  (layer2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n  )\n  (fc1): Linear(in_features=1568, out_features=32, bias=True)\n  (fc2): Linear(in_features=32, out_features=10, bias=True)\n)\nOptimizer details:\n Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    lr: 0.001\n    weight_decay: 0\n)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7f30eafb15e0>]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#################################################################################\n",
    "#########               create model and choose optimizer                ########\n",
    "#################################################################################\n",
    "model = MnistNet(config.kernels, config.dropout, config.classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss() # same as BCELoss(torch.sigmoid(x),...), but more numerically stable\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "print('Model details:\\n', model)\n",
    "print('Optimizer details:\\n',optimizer)\n",
    "\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "och : 8 [38912/60000 (65%)]\tLoss: 0.052696\t Accuracy:97.919%\n",
      "Epoch : 8 [39168/60000 (65%)]\tLoss: 0.024960\t Accuracy:97.925%\n",
      "Epoch : 8 [39424/60000 (66%)]\tLoss: 0.106863\t Accuracy:97.913%\n",
      "Epoch : 8 [39680/60000 (66%)]\tLoss: 0.056845\t Accuracy:97.922%\n",
      "Epoch : 8 [39936/60000 (67%)]\tLoss: 0.042289\t Accuracy:97.925%\n",
      "Epoch : 8 [40192/60000 (67%)]\tLoss: 0.033776\t Accuracy:97.928%\n",
      "Epoch : 8 [40448/60000 (67%)]\tLoss: 0.064082\t Accuracy:97.927%\n",
      "Epoch : 8 [40704/60000 (68%)]\tLoss: 0.074187\t Accuracy:97.935%\n",
      "Epoch : 8 [40960/60000 (68%)]\tLoss: 0.030761\t Accuracy:97.938%\n",
      "Epoch : 8 [41216/60000 (69%)]\tLoss: 0.138117\t Accuracy:97.931%\n",
      "Epoch : 8 [41472/60000 (69%)]\tLoss: 0.049155\t Accuracy:97.920%\n",
      "Epoch : 8 [41728/60000 (70%)]\tLoss: 0.014267\t Accuracy:97.916%\n",
      "Epoch : 8 [41984/60000 (70%)]\tLoss: 0.082151\t Accuracy:97.910%\n",
      "Epoch : 8 [42240/60000 (70%)]\tLoss: 0.112163\t Accuracy:97.908%\n",
      "Epoch : 8 [42496/60000 (71%)]\tLoss: 0.060565\t Accuracy:97.909%\n",
      "Epoch : 8 [42752/60000 (71%)]\tLoss: 0.020450\t Accuracy:97.912%\n",
      "Epoch : 8 [43008/60000 (72%)]\tLoss: 0.046054\t Accuracy:97.913%\n",
      "Epoch : 8 [43264/60000 (72%)]\tLoss: 0.036564\t Accuracy:97.916%\n",
      "Epoch : 8 [43520/60000 (72%)]\tLoss: 0.153853\t Accuracy:97.907%\n",
      "Epoch : 8 [43776/60000 (73%)]\tLoss: 0.051380\t Accuracy:97.913%\n",
      "Epoch : 8 [44032/60000 (73%)]\tLoss: 0.108489\t Accuracy:97.911%\n",
      "Epoch : 8 [44288/60000 (74%)]\tLoss: 0.061926\t Accuracy:97.905%\n",
      "Epoch : 8 [44544/60000 (74%)]\tLoss: 0.007650\t Accuracy:97.911%\n",
      "Epoch : 8 [44800/60000 (75%)]\tLoss: 0.056277\t Accuracy:97.911%\n",
      "Epoch : 8 [45056/60000 (75%)]\tLoss: 0.049425\t Accuracy:97.912%\n",
      "Epoch : 8 [45312/60000 (75%)]\tLoss: 0.096301\t Accuracy:97.915%\n",
      "Epoch : 8 [45568/60000 (76%)]\tLoss: 0.146915\t Accuracy:97.909%\n",
      "Epoch : 8 [45824/60000 (76%)]\tLoss: 0.048428\t Accuracy:97.906%\n",
      "Epoch : 8 [46080/60000 (77%)]\tLoss: 0.142569\t Accuracy:97.900%\n",
      "Epoch : 8 [46336/60000 (77%)]\tLoss: 0.072882\t Accuracy:97.892%\n",
      "Epoch : 8 [46592/60000 (78%)]\tLoss: 0.023836\t Accuracy:97.893%\n",
      "Epoch : 8 [46848/60000 (78%)]\tLoss: 0.147641\t Accuracy:97.883%\n",
      "Epoch : 8 [47104/60000 (78%)]\tLoss: 0.017335\t Accuracy:97.882%\n",
      "Epoch : 8 [47360/60000 (79%)]\tLoss: 0.142504\t Accuracy:97.862%\n",
      "Epoch : 8 [47616/60000 (79%)]\tLoss: 0.041327\t Accuracy:97.863%\n",
      "Epoch : 8 [47872/60000 (80%)]\tLoss: 0.098401\t Accuracy:97.862%\n",
      "Epoch : 8 [48128/60000 (80%)]\tLoss: 0.091737\t Accuracy:97.861%\n",
      "Epoch : 8 [48384/60000 (81%)]\tLoss: 0.042261\t Accuracy:97.853%\n",
      "Epoch : 8 [48640/60000 (81%)]\tLoss: 0.034437\t Accuracy:97.848%\n",
      "Epoch : 8 [48896/60000 (81%)]\tLoss: 0.050714\t Accuracy:97.853%\n",
      "Epoch : 8 [49152/60000 (82%)]\tLoss: 0.155258\t Accuracy:97.840%\n",
      "Epoch : 8 [49408/60000 (82%)]\tLoss: 0.094749\t Accuracy:97.845%\n",
      "Epoch : 8 [49664/60000 (83%)]\tLoss: 0.003503\t Accuracy:97.840%\n",
      "Epoch : 8 [49920/60000 (83%)]\tLoss: 0.179320\t Accuracy:97.833%\n",
      "Epoch : 8 [50176/60000 (84%)]\tLoss: 0.238182\t Accuracy:97.838%\n",
      "Epoch : 8 [50432/60000 (84%)]\tLoss: 0.045265\t Accuracy:97.839%\n",
      "Epoch : 8 [50688/60000 (84%)]\tLoss: 0.056358\t Accuracy:97.842%\n",
      "Epoch : 8 [50944/60000 (85%)]\tLoss: 0.016062\t Accuracy:97.840%\n",
      "Epoch : 8 [51200/60000 (85%)]\tLoss: 0.312595\t Accuracy:97.844%\n",
      "Epoch : 8 [51456/60000 (86%)]\tLoss: 0.029865\t Accuracy:97.847%\n",
      "Epoch : 8 [51712/60000 (86%)]\tLoss: 0.052470\t Accuracy:97.848%\n",
      "Epoch : 8 [51968/60000 (87%)]\tLoss: 0.175417\t Accuracy:97.851%\n",
      "Epoch : 8 [52224/60000 (87%)]\tLoss: 0.116510\t Accuracy:97.848%\n",
      "Epoch : 8 [52480/60000 (87%)]\tLoss: 0.006991\t Accuracy:97.853%\n",
      "Epoch : 8 [52736/60000 (88%)]\tLoss: 0.052283\t Accuracy:97.862%\n",
      "Epoch : 8 [52992/60000 (88%)]\tLoss: 0.073886\t Accuracy:97.849%\n",
      "Epoch : 8 [53248/60000 (89%)]\tLoss: 0.006991\t Accuracy:97.847%\n",
      "Epoch : 8 [53504/60000 (89%)]\tLoss: 0.065189\t Accuracy:97.849%\n",
      "Epoch : 8 [53760/60000 (90%)]\tLoss: 0.035791\t Accuracy:97.852%\n",
      "Epoch : 8 [54016/60000 (90%)]\tLoss: 0.057764\t Accuracy:97.853%\n",
      "Epoch : 8 [54272/60000 (90%)]\tLoss: 0.027059\t Accuracy:97.858%\n",
      "Epoch : 8 [54528/60000 (91%)]\tLoss: 0.036024\t Accuracy:97.862%\n",
      "Epoch : 8 [54784/60000 (91%)]\tLoss: 0.029251\t Accuracy:97.867%\n",
      "Epoch : 8 [55040/60000 (92%)]\tLoss: 0.036723\t Accuracy:97.869%\n",
      "Epoch : 8 [55296/60000 (92%)]\tLoss: 0.034942\t Accuracy:97.876%\n",
      "Epoch : 8 [55552/60000 (93%)]\tLoss: 0.034465\t Accuracy:97.880%\n",
      "Epoch : 8 [55808/60000 (93%)]\tLoss: 0.019976\t Accuracy:97.883%\n",
      "Epoch : 8 [56064/60000 (93%)]\tLoss: 0.007095\t Accuracy:97.889%\n",
      "Epoch : 8 [56320/60000 (94%)]\tLoss: 0.060431\t Accuracy:97.893%\n",
      "Epoch : 8 [56576/60000 (94%)]\tLoss: 0.067481\t Accuracy:97.895%\n",
      "Epoch : 8 [56832/60000 (95%)]\tLoss: 0.091438\t Accuracy:97.898%\n",
      "Epoch : 8 [57088/60000 (95%)]\tLoss: 0.008346\t Accuracy:97.902%\n",
      "Epoch : 8 [57344/60000 (96%)]\tLoss: 0.026977\t Accuracy:97.903%\n",
      "Epoch : 8 [57600/60000 (96%)]\tLoss: 0.088175\t Accuracy:97.905%\n",
      "Epoch : 8 [57856/60000 (96%)]\tLoss: 0.008539\t Accuracy:97.909%\n",
      "Epoch : 8 [58112/60000 (97%)]\tLoss: 0.021123\t Accuracy:97.908%\n",
      "Epoch : 8 [58368/60000 (97%)]\tLoss: 0.031729\t Accuracy:97.914%\n",
      "Epoch : 8 [58624/60000 (98%)]\tLoss: 0.012461\t Accuracy:97.921%\n",
      "Epoch : 8 [58880/60000 (98%)]\tLoss: 0.020924\t Accuracy:97.929%\n",
      "Epoch : 8 [59136/60000 (99%)]\tLoss: 0.007157\t Accuracy:97.938%\n",
      "Epoch : 8 [59392/60000 (99%)]\tLoss: 0.078820\t Accuracy:97.943%\n",
      "Epoch : 8 [59648/60000 (99%)]\tLoss: 0.093501\t Accuracy:97.950%\n",
      "Epoch : 8 [59904/60000 (100%)]\tLoss: 0.293454\t Accuracy:97.949%\n",
      "Epoch : 9 [0/60000 (0%)]\tLoss: 0.020472\t Accuracy:100.000%\n",
      "Epoch : 9 [256/60000 (0%)]\tLoss: 0.009155\t Accuracy:98.438%\n",
      "Epoch : 9 [512/60000 (1%)]\tLoss: 0.012975\t Accuracy:98.438%\n",
      "Epoch : 9 [768/60000 (1%)]\tLoss: 0.091217\t Accuracy:98.798%\n",
      "Epoch : 9 [1024/60000 (2%)]\tLoss: 0.065911\t Accuracy:98.346%\n",
      "Epoch : 9 [1280/60000 (2%)]\tLoss: 0.072475\t Accuracy:98.363%\n",
      "Epoch : 9 [1536/60000 (3%)]\tLoss: 0.020325\t Accuracy:98.250%\n",
      "Epoch : 9 [1792/60000 (3%)]\tLoss: 0.034196\t Accuracy:98.276%\n",
      "Epoch : 9 [2048/60000 (3%)]\tLoss: 0.063028\t Accuracy:98.201%\n",
      "Epoch : 9 [2304/60000 (4%)]\tLoss: 0.006418\t Accuracy:98.184%\n",
      "Epoch : 9 [2560/60000 (4%)]\tLoss: 0.046091\t Accuracy:98.247%\n",
      "Epoch : 9 [2816/60000 (5%)]\tLoss: 0.024105\t Accuracy:98.264%\n",
      "Epoch : 9 [3072/60000 (5%)]\tLoss: 0.009226\t Accuracy:98.310%\n",
      "Epoch : 9 [3328/60000 (6%)]\tLoss: 0.006990\t Accuracy:98.379%\n",
      "Epoch : 9 [3584/60000 (6%)]\tLoss: 0.006456\t Accuracy:98.355%\n",
      "Epoch : 9 [3840/60000 (6%)]\tLoss: 0.024423\t Accuracy:98.309%\n",
      "Epoch : 9 [4096/60000 (7%)]\tLoss: 0.029753\t Accuracy:98.341%\n",
      "Epoch : 9 [4352/60000 (7%)]\tLoss: 0.030354\t Accuracy:98.324%\n",
      "Epoch : 9 [4608/60000 (8%)]\tLoss: 0.149534\t Accuracy:98.309%\n",
      "Epoch : 9 [4864/60000 (8%)]\tLoss: 0.013597\t Accuracy:98.356%\n",
      "Epoch : 9 [5120/60000 (9%)]\tLoss: 0.159971\t Accuracy:98.322%\n",
      "Epoch : 9 [5376/60000 (9%)]\tLoss: 0.086713\t Accuracy:98.272%\n",
      "Epoch : 9 [5632/60000 (9%)]\tLoss: 0.030907\t Accuracy:98.279%\n",
      "Epoch : 9 [5888/60000 (10%)]\tLoss: 0.085528\t Accuracy:98.236%\n",
      "Epoch : 9 [6144/60000 (10%)]\tLoss: 0.017295\t Accuracy:98.244%\n",
      "Epoch : 9 [6400/60000 (11%)]\tLoss: 0.032756\t Accuracy:98.190%\n",
      "Epoch : 9 [6656/60000 (11%)]\tLoss: 0.075252\t Accuracy:98.185%\n",
      "Epoch : 9 [6912/60000 (12%)]\tLoss: 0.179773\t Accuracy:98.136%\n",
      "Epoch : 9 [7168/60000 (12%)]\tLoss: 0.035659\t Accuracy:98.106%\n",
      "Epoch : 9 [7424/60000 (12%)]\tLoss: 0.015751\t Accuracy:98.117%\n",
      "Epoch : 9 [7680/60000 (13%)]\tLoss: 0.074874\t Accuracy:98.115%\n",
      "Epoch : 9 [7936/60000 (13%)]\tLoss: 0.019408\t Accuracy:98.088%\n",
      "Epoch : 9 [8192/60000 (14%)]\tLoss: 0.047725\t Accuracy:98.110%\n",
      "Epoch : 9 [8448/60000 (14%)]\tLoss: 0.033629\t Accuracy:98.132%\n",
      "Epoch : 9 [8704/60000 (14%)]\tLoss: 0.123727\t Accuracy:98.130%\n",
      "Epoch : 9 [8960/60000 (15%)]\tLoss: 0.058753\t Accuracy:98.116%\n",
      "Epoch : 9 [9216/60000 (15%)]\tLoss: 0.080950\t Accuracy:98.136%\n",
      "Epoch : 9 [9472/60000 (16%)]\tLoss: 0.101765\t Accuracy:98.070%\n",
      "Epoch : 9 [9728/60000 (16%)]\tLoss: 0.012638\t Accuracy:98.049%\n",
      "Epoch : 9 [9984/60000 (17%)]\tLoss: 0.012991\t Accuracy:98.089%\n",
      "Epoch : 9 [10240/60000 (17%)]\tLoss: 0.205940\t Accuracy:98.059%\n",
      "Epoch : 9 [10496/60000 (17%)]\tLoss: 0.027823\t Accuracy:98.078%\n",
      "Epoch : 9 [10752/60000 (18%)]\tLoss: 0.122474\t Accuracy:98.105%\n",
      "Epoch : 9 [11008/60000 (18%)]\tLoss: 0.079380\t Accuracy:98.085%\n",
      "Epoch : 9 [11264/60000 (19%)]\tLoss: 0.017410\t Accuracy:98.093%\n",
      "Epoch : 9 [11520/60000 (19%)]\tLoss: 0.113312\t Accuracy:98.092%\n",
      "Epoch : 9 [11776/60000 (20%)]\tLoss: 0.051478\t Accuracy:98.049%\n",
      "Epoch : 9 [12032/60000 (20%)]\tLoss: 0.086721\t Accuracy:98.032%\n",
      "Epoch : 9 [12288/60000 (20%)]\tLoss: 0.045209\t Accuracy:98.033%\n",
      "Epoch : 9 [12544/60000 (21%)]\tLoss: 0.085758\t Accuracy:98.025%\n",
      "Epoch : 9 [12800/60000 (21%)]\tLoss: 0.057723\t Accuracy:97.987%\n",
      "Epoch : 9 [13056/60000 (22%)]\tLoss: 0.065742\t Accuracy:97.988%\n",
      "Epoch : 9 [13312/60000 (22%)]\tLoss: 0.030378\t Accuracy:97.981%\n",
      "Epoch : 9 [13568/60000 (23%)]\tLoss: 0.003310\t Accuracy:97.997%\n",
      "Epoch : 9 [13824/60000 (23%)]\tLoss: 0.055246\t Accuracy:97.991%\n",
      "Epoch : 9 [14080/60000 (23%)]\tLoss: 0.020870\t Accuracy:98.013%\n",
      "Epoch : 9 [14336/60000 (24%)]\tLoss: 0.123997\t Accuracy:98.007%\n",
      "Epoch : 9 [14592/60000 (24%)]\tLoss: 0.025382\t Accuracy:98.008%\n",
      "Epoch : 9 [14848/60000 (25%)]\tLoss: 0.028621\t Accuracy:98.002%\n",
      "Epoch : 9 [15104/60000 (25%)]\tLoss: 0.066777\t Accuracy:98.009%\n",
      "Epoch : 9 [15360/60000 (26%)]\tLoss: 0.053127\t Accuracy:98.016%\n",
      "Epoch : 9 [15616/60000 (26%)]\tLoss: 0.010321\t Accuracy:98.036%\n",
      "Epoch : 9 [15872/60000 (26%)]\tLoss: 0.121729\t Accuracy:98.017%\n",
      "Epoch : 9 [16128/60000 (27%)]\tLoss: 0.019499\t Accuracy:97.999%\n",
      "Epoch : 9 [16384/60000 (27%)]\tLoss: 0.007301\t Accuracy:98.024%\n",
      "Epoch : 9 [16640/60000 (28%)]\tLoss: 0.080186\t Accuracy:98.024%\n",
      "Epoch : 9 [16896/60000 (28%)]\tLoss: 0.015202\t Accuracy:98.031%\n",
      "Epoch : 9 [17152/60000 (29%)]\tLoss: 0.032474\t Accuracy:98.013%\n",
      "Epoch : 9 [17408/60000 (29%)]\tLoss: 0.012879\t Accuracy:98.031%\n",
      "Epoch : 9 [17664/60000 (29%)]\tLoss: 0.052067\t Accuracy:98.037%\n",
      "Epoch : 9 [17920/60000 (30%)]\tLoss: 0.064826\t Accuracy:98.026%\n",
      "Epoch : 9 [18176/60000 (30%)]\tLoss: 0.103647\t Accuracy:98.032%\n",
      "Epoch : 9 [18432/60000 (31%)]\tLoss: 0.008883\t Accuracy:98.037%\n",
      "Epoch : 9 [18688/60000 (31%)]\tLoss: 0.039452\t Accuracy:98.048%\n",
      "Epoch : 9 [18944/60000 (32%)]\tLoss: 0.013102\t Accuracy:98.069%\n",
      "Epoch : 9 [19200/60000 (32%)]\tLoss: 0.060101\t Accuracy:98.079%\n",
      "Epoch : 9 [19456/60000 (32%)]\tLoss: 0.044520\t Accuracy:98.084%\n",
      "Epoch : 9 [19712/60000 (33%)]\tLoss: 0.027530\t Accuracy:98.094%\n",
      "Epoch : 9 [19968/60000 (33%)]\tLoss: 0.097542\t Accuracy:98.103%\n",
      "Epoch : 9 [20224/60000 (34%)]\tLoss: 0.029446\t Accuracy:98.107%\n",
      "Epoch : 9 [20480/60000 (34%)]\tLoss: 0.004899\t Accuracy:98.126%\n",
      "Epoch : 9 [20736/60000 (35%)]\tLoss: 0.052567\t Accuracy:98.115%\n",
      "Epoch : 9 [20992/60000 (35%)]\tLoss: 0.012361\t Accuracy:98.119%\n",
      "Epoch : 9 [21248/60000 (35%)]\tLoss: 0.059012\t Accuracy:98.128%\n",
      "Epoch : 9 [21504/60000 (36%)]\tLoss: 0.040398\t Accuracy:98.127%\n",
      "Epoch : 9 [21760/60000 (36%)]\tLoss: 0.001927\t Accuracy:98.144%\n",
      "Epoch : 9 [22016/60000 (37%)]\tLoss: 0.010288\t Accuracy:98.148%\n",
      "Epoch : 9 [22272/60000 (37%)]\tLoss: 0.011543\t Accuracy:98.151%\n",
      "Epoch : 9 [22528/60000 (38%)]\tLoss: 0.140523\t Accuracy:98.150%\n",
      "Epoch : 9 [22784/60000 (38%)]\tLoss: 0.008555\t Accuracy:98.140%\n",
      "Epoch : 9 [23040/60000 (38%)]\tLoss: 0.033933\t Accuracy:98.143%\n",
      "Epoch : 9 [23296/60000 (39%)]\tLoss: 0.005208\t Accuracy:98.151%\n",
      "Epoch : 9 [23552/60000 (39%)]\tLoss: 0.128247\t Accuracy:98.154%\n",
      "Epoch : 9 [23808/60000 (40%)]\tLoss: 0.112724\t Accuracy:98.157%\n",
      "Epoch : 9 [24064/60000 (40%)]\tLoss: 0.109149\t Accuracy:98.160%\n",
      "Epoch : 9 [24320/60000 (41%)]\tLoss: 0.005814\t Accuracy:98.179%\n",
      "Epoch : 9 [24576/60000 (41%)]\tLoss: 0.054103\t Accuracy:98.194%\n",
      "Epoch : 9 [24832/60000 (41%)]\tLoss: 0.106752\t Accuracy:98.180%\n",
      "Epoch : 9 [25088/60000 (42%)]\tLoss: 0.021135\t Accuracy:98.187%\n",
      "Epoch : 9 [25344/60000 (42%)]\tLoss: 0.008771\t Accuracy:98.186%\n",
      "Epoch : 9 [25600/60000 (43%)]\tLoss: 0.172724\t Accuracy:98.184%\n",
      "Epoch : 9 [25856/60000 (43%)]\tLoss: 0.175666\t Accuracy:98.160%\n",
      "Epoch : 9 [26112/60000 (43%)]\tLoss: 0.031134\t Accuracy:98.166%\n",
      "Epoch : 9 [26368/60000 (44%)]\tLoss: 0.114898\t Accuracy:98.154%\n",
      "Epoch : 9 [26624/60000 (44%)]\tLoss: 0.137611\t Accuracy:98.138%\n",
      "Epoch : 9 [26880/60000 (45%)]\tLoss: 0.021638\t Accuracy:98.144%\n",
      "Epoch : 9 [27136/60000 (45%)]\tLoss: 0.009902\t Accuracy:98.147%\n",
      "Epoch : 9 [27392/60000 (46%)]\tLoss: 0.017300\t Accuracy:98.146%\n",
      "Epoch : 9 [27648/60000 (46%)]\tLoss: 0.030127\t Accuracy:98.138%\n",
      "Epoch : 9 [27904/60000 (46%)]\tLoss: 0.092482\t Accuracy:98.130%\n",
      "Epoch : 9 [28160/60000 (47%)]\tLoss: 0.078292\t Accuracy:98.133%\n",
      "Epoch : 9 [28416/60000 (47%)]\tLoss: 0.014633\t Accuracy:98.136%\n",
      "Epoch : 9 [28672/60000 (48%)]\tLoss: 0.138516\t Accuracy:98.138%\n",
      "Epoch : 9 [28928/60000 (48%)]\tLoss: 0.045513\t Accuracy:98.141%\n",
      "Epoch : 9 [29184/60000 (49%)]\tLoss: 0.056446\t Accuracy:98.143%\n",
      "Epoch : 9 [29440/60000 (49%)]\tLoss: 0.024426\t Accuracy:98.136%\n",
      "Epoch : 9 [29696/60000 (49%)]\tLoss: 0.146981\t Accuracy:98.132%\n",
      "Epoch : 9 [29952/60000 (50%)]\tLoss: 0.080648\t Accuracy:98.141%\n",
      "Epoch : 9 [30208/60000 (50%)]\tLoss: 0.045934\t Accuracy:98.127%\n",
      "Epoch : 9 [30464/60000 (51%)]\tLoss: 0.019268\t Accuracy:98.136%\n",
      "Epoch : 9 [30720/60000 (51%)]\tLoss: 0.087425\t Accuracy:98.122%\n",
      "Epoch : 9 [30976/60000 (52%)]\tLoss: 0.030323\t Accuracy:98.122%\n",
      "Epoch : 9 [31232/60000 (52%)]\tLoss: 0.113622\t Accuracy:98.124%\n",
      "Epoch : 9 [31488/60000 (52%)]\tLoss: 0.094913\t Accuracy:98.121%\n",
      "Epoch : 9 [31744/60000 (53%)]\tLoss: 0.122494\t Accuracy:98.101%\n",
      "Epoch : 9 [32000/60000 (53%)]\tLoss: 0.066911\t Accuracy:98.088%\n",
      "Epoch : 9 [32256/60000 (54%)]\tLoss: 0.053285\t Accuracy:98.091%\n",
      "Epoch : 9 [32512/60000 (54%)]\tLoss: 0.070738\t Accuracy:98.066%\n",
      "Epoch : 9 [32768/60000 (55%)]\tLoss: 0.034626\t Accuracy:98.072%\n",
      "Epoch : 9 [33024/60000 (55%)]\tLoss: 0.063711\t Accuracy:98.069%\n",
      "Epoch : 9 [33280/60000 (55%)]\tLoss: 0.095029\t Accuracy:98.069%\n",
      "Epoch : 9 [33536/60000 (56%)]\tLoss: 0.005191\t Accuracy:98.074%\n",
      "Epoch : 9 [33792/60000 (56%)]\tLoss: 0.004773\t Accuracy:98.083%\n",
      "Epoch : 9 [34048/60000 (57%)]\tLoss: 0.047157\t Accuracy:98.089%\n",
      "Epoch : 9 [34304/60000 (57%)]\tLoss: 0.031714\t Accuracy:98.097%\n",
      "Epoch : 9 [34560/60000 (58%)]\tLoss: 0.029444\t Accuracy:98.097%\n",
      "Epoch : 9 [34816/60000 (58%)]\tLoss: 0.258790\t Accuracy:98.082%\n",
      "Epoch : 9 [35072/60000 (58%)]\tLoss: 0.005770\t Accuracy:98.085%\n",
      "Epoch : 9 [35328/60000 (59%)]\tLoss: 0.024345\t Accuracy:98.081%\n",
      "Epoch : 9 [35584/60000 (59%)]\tLoss: 0.108154\t Accuracy:98.084%\n",
      "Epoch : 9 [35840/60000 (60%)]\tLoss: 0.167393\t Accuracy:98.081%\n",
      "Epoch : 9 [36096/60000 (60%)]\tLoss: 0.217830\t Accuracy:98.075%\n",
      "Epoch : 9 [36352/60000 (61%)]\tLoss: 0.012179\t Accuracy:98.086%\n",
      "Epoch : 9 [36608/60000 (61%)]\tLoss: 0.079586\t Accuracy:98.091%\n",
      "Epoch : 9 [36864/60000 (61%)]\tLoss: 0.014386\t Accuracy:98.091%\n",
      "Epoch : 9 [37120/60000 (62%)]\tLoss: 0.094303\t Accuracy:98.083%\n",
      "Epoch : 9 [37376/60000 (62%)]\tLoss: 0.212546\t Accuracy:98.077%\n",
      "Epoch : 9 [37632/60000 (63%)]\tLoss: 0.088949\t Accuracy:98.079%\n",
      "Epoch : 9 [37888/60000 (63%)]\tLoss: 0.013870\t Accuracy:98.082%\n",
      "Epoch : 9 [38144/60000 (64%)]\tLoss: 0.025682\t Accuracy:98.084%\n",
      "Epoch : 9 [38400/60000 (64%)]\tLoss: 0.080533\t Accuracy:98.076%\n",
      "Epoch : 9 [38656/60000 (64%)]\tLoss: 0.096790\t Accuracy:98.081%\n",
      "Epoch : 9 [38912/60000 (65%)]\tLoss: 0.016054\t Accuracy:98.086%\n",
      "Epoch : 9 [39168/60000 (65%)]\tLoss: 0.018792\t Accuracy:98.093%\n",
      "Epoch : 9 [39424/60000 (66%)]\tLoss: 0.117798\t Accuracy:98.083%\n",
      "Epoch : 9 [39680/60000 (66%)]\tLoss: 0.011832\t Accuracy:98.093%\n",
      "Epoch : 9 [39936/60000 (67%)]\tLoss: 0.069435\t Accuracy:98.093%\n",
      "Epoch : 9 [40192/60000 (67%)]\tLoss: 0.021153\t Accuracy:98.095%\n",
      "Epoch : 9 [40448/60000 (67%)]\tLoss: 0.046965\t Accuracy:98.094%\n",
      "Epoch : 9 [40704/60000 (68%)]\tLoss: 0.087529\t Accuracy:98.104%\n",
      "Epoch : 9 [40960/60000 (68%)]\tLoss: 0.179097\t Accuracy:98.099%\n",
      "Epoch : 9 [41216/60000 (69%)]\tLoss: 0.227794\t Accuracy:98.094%\n",
      "Epoch : 9 [41472/60000 (69%)]\tLoss: 0.118099\t Accuracy:98.084%\n",
      "Epoch : 9 [41728/60000 (70%)]\tLoss: 0.019748\t Accuracy:98.088%\n",
      "Epoch : 9 [41984/60000 (70%)]\tLoss: 0.021815\t Accuracy:98.093%\n",
      "Epoch : 9 [42240/60000 (70%)]\tLoss: 0.028366\t Accuracy:98.085%\n",
      "Epoch : 9 [42496/60000 (71%)]\tLoss: 0.027561\t Accuracy:98.087%\n",
      "Epoch : 9 [42752/60000 (71%)]\tLoss: 0.011999\t Accuracy:98.082%\n",
      "Epoch : 9 [43008/60000 (72%)]\tLoss: 0.080147\t Accuracy:98.078%\n",
      "Epoch : 9 [43264/60000 (72%)]\tLoss: 0.011581\t Accuracy:98.084%\n",
      "Epoch : 9 [43520/60000 (72%)]\tLoss: 0.042625\t Accuracy:98.086%\n",
      "Epoch : 9 [43776/60000 (73%)]\tLoss: 0.066716\t Accuracy:98.093%\n",
      "Epoch : 9 [44032/60000 (73%)]\tLoss: 0.024206\t Accuracy:98.104%\n",
      "Epoch : 9 [44288/60000 (74%)]\tLoss: 0.028108\t Accuracy:98.111%\n",
      "Epoch : 9 [44544/60000 (74%)]\tLoss: 0.028016\t Accuracy:98.112%\n",
      "Epoch : 9 [44800/60000 (75%)]\tLoss: 0.202376\t Accuracy:98.110%\n",
      "Epoch : 9 [45056/60000 (75%)]\tLoss: 0.060870\t Accuracy:98.105%\n",
      "Epoch : 9 [45312/60000 (75%)]\tLoss: 0.182349\t Accuracy:98.107%\n",
      "Epoch : 9 [45568/60000 (76%)]\tLoss: 0.092769\t Accuracy:98.100%\n",
      "Epoch : 9 [45824/60000 (76%)]\tLoss: 0.081937\t Accuracy:98.102%\n",
      "Epoch : 9 [46080/60000 (77%)]\tLoss: 0.093372\t Accuracy:98.106%\n",
      "Epoch : 9 [46336/60000 (77%)]\tLoss: 0.159337\t Accuracy:98.097%\n",
      "Epoch : 9 [46592/60000 (78%)]\tLoss: 0.022864\t Accuracy:98.101%\n",
      "Epoch : 9 [46848/60000 (78%)]\tLoss: 0.172379\t Accuracy:98.096%\n",
      "Epoch : 9 [47104/60000 (78%)]\tLoss: 0.016085\t Accuracy:98.098%\n",
      "Epoch : 9 [47360/60000 (79%)]\tLoss: 0.064552\t Accuracy:98.085%\n",
      "Epoch : 9 [47616/60000 (79%)]\tLoss: 0.021325\t Accuracy:98.089%\n",
      "Epoch : 9 [47872/60000 (80%)]\tLoss: 0.146186\t Accuracy:98.087%\n",
      "Epoch : 9 [48128/60000 (80%)]\tLoss: 0.107758\t Accuracy:98.093%\n",
      "Epoch : 9 [48384/60000 (81%)]\tLoss: 0.062465\t Accuracy:98.095%\n",
      "Epoch : 9 [48640/60000 (81%)]\tLoss: 0.036468\t Accuracy:98.095%\n",
      "Epoch : 9 [48896/60000 (81%)]\tLoss: 0.136916\t Accuracy:98.096%\n",
      "Epoch : 9 [49152/60000 (82%)]\tLoss: 0.264713\t Accuracy:98.088%\n",
      "Epoch : 9 [49408/60000 (82%)]\tLoss: 0.042431\t Accuracy:98.084%\n",
      "Epoch : 9 [49664/60000 (83%)]\tLoss: 0.047141\t Accuracy:98.080%\n",
      "Epoch : 9 [49920/60000 (83%)]\tLoss: 0.032763\t Accuracy:98.077%\n",
      "Epoch : 9 [50176/60000 (84%)]\tLoss: 0.192721\t Accuracy:98.071%\n",
      "Epoch : 9 [50432/60000 (84%)]\tLoss: 0.052988\t Accuracy:98.063%\n",
      "Epoch : 9 [50688/60000 (84%)]\tLoss: 0.071796\t Accuracy:98.065%\n",
      "Epoch : 9 [50944/60000 (85%)]\tLoss: 0.017075\t Accuracy:98.065%\n",
      "Epoch : 9 [51200/60000 (85%)]\tLoss: 0.147278\t Accuracy:98.067%\n",
      "Epoch : 9 [51456/60000 (86%)]\tLoss: 0.019470\t Accuracy:98.071%\n",
      "Epoch : 9 [51712/60000 (86%)]\tLoss: 0.076782\t Accuracy:98.071%\n",
      "Epoch : 9 [51968/60000 (87%)]\tLoss: 0.066283\t Accuracy:98.074%\n",
      "Epoch : 9 [52224/60000 (87%)]\tLoss: 0.089738\t Accuracy:98.070%\n",
      "Epoch : 9 [52480/60000 (87%)]\tLoss: 0.007102\t Accuracy:98.078%\n",
      "Epoch : 9 [52736/60000 (88%)]\tLoss: 0.010974\t Accuracy:98.083%\n",
      "Epoch : 9 [52992/60000 (88%)]\tLoss: 0.030187\t Accuracy:98.072%\n",
      "Epoch : 9 [53248/60000 (89%)]\tLoss: 0.046087\t Accuracy:98.074%\n",
      "Epoch : 9 [53504/60000 (89%)]\tLoss: 0.013717\t Accuracy:98.077%\n",
      "Epoch : 9 [53760/60000 (90%)]\tLoss: 0.122770\t Accuracy:98.081%\n",
      "Epoch : 9 [54016/60000 (90%)]\tLoss: 0.085094\t Accuracy:98.073%\n",
      "Epoch : 9 [54272/60000 (90%)]\tLoss: 0.019827\t Accuracy:98.079%\n",
      "Epoch : 9 [54528/60000 (91%)]\tLoss: 0.076679\t Accuracy:98.077%\n",
      "Epoch : 9 [54784/60000 (91%)]\tLoss: 0.034759\t Accuracy:98.078%\n",
      "Epoch : 9 [55040/60000 (92%)]\tLoss: 0.068476\t Accuracy:98.080%\n",
      "Epoch : 9 [55296/60000 (92%)]\tLoss: 0.165367\t Accuracy:98.080%\n",
      "Epoch : 9 [55552/60000 (93%)]\tLoss: 0.150871\t Accuracy:98.080%\n",
      "Epoch : 9 [55808/60000 (93%)]\tLoss: 0.030731\t Accuracy:98.083%\n",
      "Epoch : 9 [56064/60000 (93%)]\tLoss: 0.028563\t Accuracy:98.085%\n",
      "Epoch : 9 [56320/60000 (94%)]\tLoss: 0.015271\t Accuracy:98.081%\n",
      "Epoch : 9 [56576/60000 (94%)]\tLoss: 0.085736\t Accuracy:98.079%\n",
      "Epoch : 9 [56832/60000 (95%)]\tLoss: 0.005987\t Accuracy:98.082%\n",
      "Epoch : 9 [57088/60000 (95%)]\tLoss: 0.011030\t Accuracy:98.088%\n",
      "Epoch : 9 [57344/60000 (96%)]\tLoss: 0.061968\t Accuracy:98.086%\n",
      "Epoch : 9 [57600/60000 (96%)]\tLoss: 0.057257\t Accuracy:98.089%\n",
      "Epoch : 9 [57856/60000 (96%)]\tLoss: 0.092117\t Accuracy:98.087%\n",
      "Epoch : 9 [58112/60000 (97%)]\tLoss: 0.009234\t Accuracy:98.089%\n",
      "Epoch : 9 [58368/60000 (97%)]\tLoss: 0.009560\t Accuracy:98.094%\n",
      "Epoch : 9 [58624/60000 (98%)]\tLoss: 0.035982\t Accuracy:98.097%\n",
      "Epoch : 9 [58880/60000 (98%)]\tLoss: 0.005936\t Accuracy:98.102%\n",
      "Epoch : 9 [59136/60000 (99%)]\tLoss: 0.002045\t Accuracy:98.110%\n",
      "Epoch : 9 [59392/60000 (99%)]\tLoss: 0.085172\t Accuracy:98.111%\n",
      "Epoch : 9 [59648/60000 (99%)]\tLoss: 0.042970\t Accuracy:98.114%\n",
      "Epoch : 9 [59904/60000 (100%)]\tLoss: 0.389987\t Accuracy:98.111%\n",
      "--- 4.206118476390839 minutes ---\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "#########                   start the training process                   ########\n",
    "#################################################################################\n",
    "# calculate the time for the code execution\n",
    "start_time = tt.time()\n",
    "# switch model into train mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(config.epoch):\n",
    "    correct = 0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_data_loader):\n",
    "        var_X_batch = torch.autograd.Variable(X_batch).float()\n",
    "        var_y_batch = torch.autograd.Variable(y_batch).long()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(var_X_batch).squeeze(1)\n",
    "        loss = criterion(output, var_y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Total correct predictions\n",
    "        predicted = torch.max(output.data, 1)[1] \n",
    "        #predicted = torch.round(torch.sigmoid(output))\n",
    "        correct += (predicted == var_y_batch).sum()\n",
    "        #print(correct)\n",
    "        wandb.log({'train_loss': loss.item(), 'train_acc': float(correct*100) / float(config.batch_size*(batch_idx+1))})\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                  epoch, batch_idx*len(X_batch), len(train_data_loader.dataset), 100.* batch_idx / len(train_data_loader),\n",
    "                  loss.item(), float(correct*100) / float(config.batch_size*(batch_idx+1))))\n",
    "\n",
    "# save the general checkpoint\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item()\n",
    "            }, os.path.join(output_path,'mnistnet_training_checkpoint.pt'))\n",
    "\n",
    "print (\"--- %s minutes ---\" % ((tt.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test accuracy:98.900% \n",
      "Test accuracy:98.750% \n",
      "Test accuracy:98.600% \n",
      "Test accuracy:98.750% \n",
      "Test accuracy:98.740% \n",
      "Test accuracy:98.900% \n",
      "Test accuracy:98.986% \n",
      "Test accuracy:99.100% \n",
      "Test accuracy:99.156% \n",
      "Test accuracy:99.140% \n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "#########                  start the evaluation process                  ########\n",
    "#################################################################################\n",
    "# switch model into evaluation mode\n",
    "model.eval()\n",
    "correct = 0\n",
    "for batch_idx, (test_imgs, test_labels) in enumerate(test_data_loader):\n",
    "    test_imgs = torch.autograd.Variable(test_imgs).float()\n",
    "    output = model(test_imgs)\n",
    "    predicted = torch.max(output,1)[1]\n",
    "    #predicted = torch.round(torch.sigmoid(output))\n",
    "    correct += (predicted == test_labels).sum()\n",
    "    #print(\"Test accuracy:{:.3f}% \".format(float(correct) / (len(test_data_loader)*config.batch_size)))\n",
    "    print(\"Test accuracy:{:.3f}% \".format(float(correct*100) / float(batch_size_test*(batch_idx+1))))\n",
    "    wandb.log({'test_acc': float(correct*100) / float(config.batch_size*(batch_idx+1))})\n",
    "    \n"
   ]
  }
 ]
}
