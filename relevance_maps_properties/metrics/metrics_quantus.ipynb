{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"150\" alt=\"Logo_ER10\" src=\"https://user-images.githubusercontent.com/3244249/151994514-b584b984-a148-4ade-80ee-0f88b0aefa45.png\">\n",
    "\n",
    "### Using Quantus Evaluation with DIANNA\n",
    "This notebook demonstrates the use of [Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus) with the use of DIANNA's explanation methods. <br> We will demonstrate the necessary input and output constraints and postprocessing that is necessary to ensure integration between the two libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # disable warnings relateds to versions of tf\n",
    "import numpy as np\n",
    "import dianna\n",
    "import onnx\n",
    "import quantus\n",
    "\n",
    "from metrics import Incremental_deletion\n",
    "from utils import * # Imports utilities for quantus compatability\n",
    "\n",
    "from onnx_tf.backend import prepare\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Loading in the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load dataset\n",
    "data = np.load(Path().absolute().parent / 'data/binary-mnist.npz')\n",
    "# load testing data and the related labels\n",
    "X_test = data['X_test'].astype(np.float32).reshape([-1, 28, 28, 1]) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved onnx model\n",
    "onnx_model_path = Path('../models', 'mnist_model_tf.onnx')\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "# get the output node\n",
    "output_node = prepare(onnx_model, gen_tensor_dict=True).outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: digit 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3dc72e5cd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# class name\n",
    "class_name = ['digit 0', 'digit 1']\n",
    "# instance index\n",
    "i_instance = 1\n",
    "# select instance for testing\n",
    "test_sample = X_test[i_instance].copy().astype(np.float32)\n",
    "# model predictions with added batch axis to test sample\n",
    "predictions = prepare(onnx_model).run(test_sample[None, ...])[f'{output_node}']\n",
    "pred_class = class_name[np.argmax(predictions)]\n",
    "print(\"The predicted class is:\", pred_class)\n",
    "plt.imshow(X_test[i_instance][:,:,0], cmap='gray')  # 0 for channel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Batching the input \n",
    "\n",
    "Quantus requires batched input variables, which we introduce here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch = test_sample[np.newaxis, ...]\n",
    "y_batch = np.array([0])\n",
    "\n",
    "x_batch.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Checking available methods\n",
    "\n",
    "Below we display all the available methods in quantus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Faithfulness',\n",
       " 'Robustness',\n",
       " 'Localisation',\n",
       " 'Complexity',\n",
       " 'Randomisation',\n",
       " 'Axiomatic']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantus.helpers.constants.available_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Faithfulness': ['Faithfulness Correlation',\n",
       "  'Faithfulness Estimate',\n",
       "  'Pixel-Flipping',\n",
       "  'Region Segmentation',\n",
       "  'Monotonicity-Arya',\n",
       "  'Monotonicity-Nguyen',\n",
       "  'Selectivity',\n",
       "  'SensitivityN',\n",
       "  'IROF',\n",
       "  'ROAD',\n",
       "  'Infidelity',\n",
       "  'Sufficiency'],\n",
       " 'Robustness': ['Continuity Test',\n",
       "  'Local Lipschitz Estimate',\n",
       "  'Max-Sensitivity',\n",
       "  'Avg-Sensitivity',\n",
       "  'Consistency',\n",
       "  'Relative Input Stability',\n",
       "  'Relative Output Stability',\n",
       "  'Relative Representation Stability'],\n",
       " 'Localisation': ['Pointing Game',\n",
       "  'Top-K Intersection',\n",
       "  'Relevance Mass Accuracy',\n",
       "  'Relevance Rank Accuracy',\n",
       "  'Attribution Localisation ',\n",
       "  'AUC',\n",
       "  'Focus'],\n",
       " 'Complexity': ['Sparseness', 'Complexity', 'Effective Complexity'],\n",
       " 'Randomisation': ['Model Parameter Randomisation', 'Random Logit'],\n",
       " 'Axiomatic': ['Completeness', 'NonSensitivity', 'InputInvariance']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantus.helpers.constants.available_metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - ONNX to Keras\n",
    "\n",
    "This is an experimental feature so keep in mind to **USE THIS WITH CARE**. This conversion requires the input names defined before pickling the model. <br> I have included a helper function that reads this name from the onnx_model as `get_onnx_names`. We then leverage the `onnx_to_keras` library that does the heavy lifting of conversion for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to use `same` padding. Add ZeroPadding2D layer to fix shapes.\n",
      "Failed to use keras.layers.Add. Fallback to TF lambda.\n",
      "Failed to use keras.layers.Add. Fallback to TF lambda.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(None, 128), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(128,), dtype=float32)\n",
      "Tensor(\"Placeholder:0\", shape=(None, 2), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from onnx2keras import onnx_to_keras\n",
    "\n",
    "feed_net_input, output = get_onnx_names(onnx_model)\n",
    "k_model = onnx_to_keras(onnx_model, net_feed_input, name_policy='renumerate', verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Using the metrics!\n",
    "\n",
    "Finally, we show how to use some of the metrics. Keep in mind that the integration between the two libraries is not particularly stable and therefore not all of the afformentioned metrics work. <br> Aditionally, it is vital to use the postprocess functions in order for the metrics to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Avg Sensitivity metric is likely to be sensitive to the choice of amount of noise added 'lower_bound' and 'upper_bound', the number of samples iterated over 'nr_samples', the function to perturb the input 'perturb_func', the similarity metric 'similarity_func' as well as norm calculations on the numerator and denominator of the sensitivity equation i.e., 'norm_numerator' and 'norm_denominator'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Yeh, Chih-Kuan, et al. 'On the (in) fidelity and sensitivity for explanations.' arXiv preprint arXiv:1901.09392 (2019).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.83s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.85s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.81s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.84s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.74s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.002556783560096721]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average Sensitivity: KernelSHAP\n",
    "quantus.AvgSensitivity(\n",
    "    nr_samples=5,\n",
    "    perturb_func=quantus.perturb_func.gaussian_noise,\n",
    ")(model=k_model, \n",
    "   x_batch=x_batch,\n",
    "   y_batch=y_batch,\n",
    "   a_batch=None,   \n",
    "   explain_func=SHAP_postprocess, \n",
    "   explain_func_kwargs={'model_or_function': onnx_model_path,\n",
    "                        \"input_data\": test_sample,\n",
    "                        \"labels\": range(1),\n",
    "                       }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Avg Sensitivity metric is likely to be sensitive to the choice of amount of noise added 'lower_bound' and 'upper_bound', the number of samples iterated over 'nr_samples', the function to perturb the input 'perturb_func', the similarity metric 'similarity_func' as well as norm calculations on the numerator and denominator of the sensitivity equation i.e., 'norm_numerator' and 'norm_denominator'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Yeh, Chih-Kuan, et al. 'On the (in) fidelity and sensitivity for explanations.' arXiv preprint arXiv:1901.09392 (2019).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:07<00:00, 704.54it/s]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 631.43it/s]\n",
      "100%|██████████| 5000/5000 [00:08<00:00, 594.72it/s]\n",
      "100%|██████████| 5000/5000 [00:08<00:00, 620.82it/s]\n",
      " 95%|█████████▌| 4774/5000 [00:07<00:00, 683.47it/s]"
     ]
    }
   ],
   "source": [
    "# Average Sensitivity: LIME\n",
    "quantus.AvgSensitivity(\n",
    "    nr_samples=5,\n",
    "    perturb_func=quantus.perturb_func.gaussian_noise,\n",
    ")(model=k_model, \n",
    "   x_batch=x_batch,\n",
    "   y_batch=y_batch,\n",
    "   a_batch=None,   \n",
    "   explain_func=LIME_postprocess, \n",
    "   explain_func_kwargs={'model_or_function': onnx_model_path,\n",
    "                        \"input_data\": test_sample,\n",
    "                        \"labels\": range(1),\n",
    "                       }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Sensitivity: RISE\n",
    "quantus.AvgSensitivity(\n",
    "    nr_samples=5,\n",
    "    perturb_func=quantus.perturb_func.gaussian_noise,\n",
    ")(model=k_model, \n",
    "   x_batch=x_batch,\n",
    "   y_batch=y_batch,\n",
    "   a_batch=None,   \n",
    "   explain_func=dianna.explain_image, \n",
    "   explain_func_kwargs={'model_or_function': onnx_model_path,\n",
    "                        \"input_data\": test_sample,\n",
    "                        \"method\": \"RISE\",\n",
    "                        \"labels\": range(1),\n",
    "                       }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
