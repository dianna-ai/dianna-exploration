{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"150\" alt=\"Logo_ER10\" src=\"https://user-images.githubusercontent.com/3244249/151994514-b584b984-a148-4ade-80ee-0f88b0aefa45.png\">\n",
    "\n",
    "### Using Quantus Evaluation with DIANNA\n",
    "This notebook demonstrates the use of [Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus) with the use of DIANNA's explanation methods. <br> We will demonstrate the necessary input and output constraints and postprocessing that is necessary to ensure integration between the two libraries. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 15:11:36.495588: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-01 15:11:36.547759: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-01 15:11:36.548851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-01 15:11:37.300113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # disable warnings relateds to versions of tf\n",
    "import numpy as np\n",
    "import dianna\n",
    "import onnx\n",
    "import quantus\n",
    "\n",
    "from metrics import Incremental_deletion\n",
    "from utils import * # Imports utilities for quantus compatability\n",
    "\n",
    "from onnx_tf.backend import prepare\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Loading in the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 24,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load dataset\n",
    "data = np.load(Path().absolute().parent / 'data/binary-mnist.npz')\n",
    "# load testing data and the related labels\n",
    "X_test = data['X_test'].astype(np.float32).reshape([-1, 28, 28, 1]) / 255"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 25,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved onnx model\n",
    "onnx_model_path = Path('../models', 'mnist_model_tf.onnx')\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "# get the output node\n",
    "output_node = prepare(onnx_model, gen_tensor_dict=True).outputs[0]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 26,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: digit 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "<matplotlib.image.AxesImage at 0x7f428866ad60>"
      ]
     },
     "execution_count": 4,
=======
       "<matplotlib.image.AxesImage at 0x7f3dc72e5cd0>"
      ]
     },
     "execution_count": 26,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# class name\n",
    "class_name = ['digit 0', 'digit 1']\n",
    "# instance index\n",
    "i_instance = 1\n",
    "# select instance for testing\n",
    "test_sample = X_test[i_instance].copy().astype(np.float32)\n",
    "# model predictions with added batch axis to test sample\n",
    "predictions = prepare(onnx_model).run(test_sample[None, ...])[f'{output_node}']\n",
    "pred_class = class_name[np.argmax(predictions)]\n",
    "print(\"The predicted class is:\", pred_class)\n",
    "plt.imshow(X_test[i_instance][:,:,0], cmap='gray')  # 0 for channel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Batching the input \n",
    "\n",
    "Quantus requires batched input variables, which we introduce here. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 32,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 5,
=======
     "execution_count": 32,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch = test_sample[np.newaxis, ...]\n",
    "y_batch = np.array([0])\n",
    "\n",
    "x_batch.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Checking available methods\n",
    "\n",
    "Below we display all the available methods in quantus"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 33,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Faithfulness',\n",
       " 'Robustness',\n",
       " 'Localisation',\n",
       " 'Complexity',\n",
       " 'Randomisation',\n",
       " 'Axiomatic']"
      ]
     },
<<<<<<< HEAD
     "execution_count": 6,
=======
     "execution_count": 33,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantus.helpers.constants.available_categories()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 34,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Faithfulness': ['Faithfulness Correlation',\n",
       "  'Faithfulness Estimate',\n",
       "  'Pixel-Flipping',\n",
       "  'Region Segmentation',\n",
       "  'Monotonicity-Arya',\n",
       "  'Monotonicity-Nguyen',\n",
       "  'Selectivity',\n",
       "  'SensitivityN',\n",
       "  'IROF',\n",
       "  'ROAD',\n",
       "  'Infidelity',\n",
       "  'Sufficiency'],\n",
       " 'Robustness': ['Continuity Test',\n",
       "  'Local Lipschitz Estimate',\n",
       "  'Max-Sensitivity',\n",
       "  'Avg-Sensitivity',\n",
       "  'Consistency',\n",
       "  'Relative Input Stability',\n",
       "  'Relative Output Stability',\n",
       "  'Relative Representation Stability'],\n",
       " 'Localisation': ['Pointing Game',\n",
       "  'Top-K Intersection',\n",
       "  'Relevance Mass Accuracy',\n",
       "  'Relevance Rank Accuracy',\n",
       "  'Attribution Localisation ',\n",
       "  'AUC',\n",
       "  'Focus'],\n",
       " 'Complexity': ['Sparseness', 'Complexity', 'Effective Complexity'],\n",
       " 'Randomisation': ['Model Parameter Randomisation', 'Random Logit'],\n",
       " 'Axiomatic': ['Completeness', 'NonSensitivity', 'InputInvariance']}"
      ]
     },
<<<<<<< HEAD
     "execution_count": 7,
=======
     "execution_count": 34,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantus.helpers.constants.available_metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - ONNX to Keras\n",
    "\n",
    "This is an experimental feature so keep in mind to **USE THIS WITH CARE**. This conversion requires the input names defined before pickling the model. <br> I have included a helper function that reads this name from the onnx_model as `get_onnx_names`. We then leverage the `onnx_to_keras` library that does the heavy lifting of conversion for us. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 50,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to use `same` padding. Add ZeroPadding2D layer to fix shapes.\n",
      "Failed to use keras.layers.Add. Fallback to TF lambda.\n",
      "Failed to use keras.layers.Add. Fallback to TF lambda.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(None, 128), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(128,), dtype=float32)\n",
      "Tensor(\"Placeholder:0\", shape=(None, 2), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from onnx2keras import onnx_to_keras\n",
    "\n",
<<<<<<< HEAD
    "net_feed_input, output = get_onnx_names(onnx_model)\n",
=======
    "feed_net_input, output = get_onnx_names(onnx_model)\n",
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
    "k_model = onnx_to_keras(onnx_model, net_feed_input, name_policy='renumerate', verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Using the metrics!\n",
    "\n",
    "Finally, we show how to use some of the metrics. Keep in mind that the integration between the two libraries is not particularly stable and therefore not all of the afformentioned metrics work. <br> Aditionally, it is vital to use the postprocess functions in order for the metrics to work. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 49,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Avg Sensitivity metric is likely to be sensitive to the choice of amount of noise added 'lower_bound' and 'upper_bound', the number of samples iterated over 'nr_samples', the function to perturb the input 'perturb_func', the similarity metric 'similarity_func' as well as norm calculations on the numerator and denominator of the sensitivity equation i.e., 'norm_numerator' and 'norm_denominator'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Yeh, Chih-Kuan, et al. 'On the (in) fidelity and sensitivity for explanations.' arXiv preprint arXiv:1901.09392 (2019).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
<<<<<<< HEAD
     "ename": "TypeError",
     "evalue": "explain() got multiple values for argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Average Sensitivity: KernelSHAP\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m quantus\u001b[39m.\u001b[39;49mAvgSensitivity(\n\u001b[1;32m      3\u001b[0m     nr_samples\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     perturb_func\u001b[39m=\u001b[39;49mquantus\u001b[39m.\u001b[39;49mperturb_func\u001b[39m.\u001b[39;49mgaussian_noise,\n\u001b[1;32m      5\u001b[0m )(model\u001b[39m=\u001b[39;49mk_model, \n\u001b[1;32m      6\u001b[0m    x_batch\u001b[39m=\u001b[39;49mx_batch,\n\u001b[1;32m      7\u001b[0m    y_batch\u001b[39m=\u001b[39;49my_batch,\n\u001b[1;32m      8\u001b[0m    a_batch\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,   \n\u001b[1;32m      9\u001b[0m    explain_func\u001b[39m=\u001b[39;49mSHAP_postprocess, \n\u001b[1;32m     10\u001b[0m    explain_func_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mmodel_or_function\u001b[39;49m\u001b[39m'\u001b[39;49m: onnx_model_path,\n\u001b[1;32m     11\u001b[0m                         \u001b[39m\"\u001b[39;49m\u001b[39minput_data\u001b[39;49m\u001b[39m\"\u001b[39;49m: test_sample,\n\u001b[1;32m     12\u001b[0m                         \u001b[39m\"\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mrange\u001b[39;49m(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m     13\u001b[0m                         \u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0\u001b[39;49m\n\u001b[1;32m     14\u001b[0m                        }\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Thesis/dianna-exploration/venv/lib/python3.9/site-packages/quantus/metrics/robustness/avg_sensitivity.py:254\u001b[0m, in \u001b[0;36mAvgSensitivity.__call__\u001b[0;34m(self, model, x_batch, y_batch, a_batch, s_batch, channel_first, explain_func, explain_func_kwargs, model_predict_kwargs, softmax, device, batch_size, custom_batch, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    165\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    166\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    180\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    181\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m    This implementation represents the main logic of the metric and makes the class object callable.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m    It completes instance-wise evaluation of explanations (a_batch) with respect to input data (x_batch),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39m        >> scores = metric(model=model, x_batch=x_batch, y_batch=y_batch, a_batch=a_batch_saliency}\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\n\u001b[1;32m    255\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    256\u001b[0m         x_batch\u001b[39m=\u001b[39;49mx_batch,\n\u001b[1;32m    257\u001b[0m         y_batch\u001b[39m=\u001b[39;49my_batch,\n\u001b[1;32m    258\u001b[0m         a_batch\u001b[39m=\u001b[39;49ma_batch,\n\u001b[1;32m    259\u001b[0m         s_batch\u001b[39m=\u001b[39;49ms_batch,\n\u001b[1;32m    260\u001b[0m         custom_batch\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    261\u001b[0m         channel_first\u001b[39m=\u001b[39;49mchannel_first,\n\u001b[1;32m    262\u001b[0m         explain_func\u001b[39m=\u001b[39;49mexplain_func,\n\u001b[1;32m    263\u001b[0m         explain_func_kwargs\u001b[39m=\u001b[39;49mexplain_func_kwargs,\n\u001b[1;32m    264\u001b[0m         softmax\u001b[39m=\u001b[39;49msoftmax,\n\u001b[1;32m    265\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    266\u001b[0m         model_predict_kwargs\u001b[39m=\u001b[39;49mmodel_predict_kwargs,\n\u001b[1;32m    267\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    268\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    269\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Thesis/dianna-exploration/venv/lib/python3.9/site-packages/quantus/metrics/base_batched.py:194\u001b[0m, in \u001b[0;36mBatchedMetric.__call__\u001b[0;34m(self, model, x_batch, y_batch, a_batch, s_batch, channel_first, explain_func, explain_func_kwargs, model_predict_kwargs, softmax, device, batch_size, custom_batch, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m warn\u001b[39m.\u001b[39mdeprecation_warnings(kwargs)\n\u001b[1;32m    192\u001b[0m warn\u001b[39m.\u001b[39mcheck_kwargs(kwargs)\n\u001b[0;32m--> 194\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgeneral_preprocess(\n\u001b[1;32m    195\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    196\u001b[0m     x_batch\u001b[39m=\u001b[39;49mx_batch,\n\u001b[1;32m    197\u001b[0m     y_batch\u001b[39m=\u001b[39;49my_batch,\n\u001b[1;32m    198\u001b[0m     a_batch\u001b[39m=\u001b[39;49ma_batch,\n\u001b[1;32m    199\u001b[0m     s_batch\u001b[39m=\u001b[39;49ms_batch,\n\u001b[1;32m    200\u001b[0m     custom_batch\u001b[39m=\u001b[39;49mcustom_batch,\n\u001b[1;32m    201\u001b[0m     channel_first\u001b[39m=\u001b[39;49mchannel_first,\n\u001b[1;32m    202\u001b[0m     explain_func\u001b[39m=\u001b[39;49mexplain_func,\n\u001b[1;32m    203\u001b[0m     explain_func_kwargs\u001b[39m=\u001b[39;49mexplain_func_kwargs,\n\u001b[1;32m    204\u001b[0m     model_predict_kwargs\u001b[39m=\u001b[39;49mmodel_predict_kwargs,\n\u001b[1;32m    205\u001b[0m     softmax\u001b[39m=\u001b[39;49msoftmax,\n\u001b[1;32m    206\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    207\u001b[0m )\n\u001b[1;32m    209\u001b[0m \u001b[39m# Create generator for generating batches.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m batch_generator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_batches(\n\u001b[1;32m    211\u001b[0m     data\u001b[39m=\u001b[39mdata,\n\u001b[1;32m    212\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    213\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Thesis/dianna-exploration/venv/lib/python3.9/site-packages/quantus/metrics/base.py:383\u001b[0m, in \u001b[0;36mMetric.general_preprocess\u001b[0;34m(self, model, x_batch, y_batch, a_batch, s_batch, channel_first, explain_func, explain_func_kwargs, model_predict_kwargs, softmax, device, custom_batch)\u001b[0m\n\u001b[1;32m    380\u001b[0m     asserts\u001b[39m.\u001b[39massert_explain_func(explain_func\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplain_func)\n\u001b[1;32m    382\u001b[0m     \u001b[39m# Generate explanations.\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m     a_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplain_func(\n\u001b[1;32m    384\u001b[0m         model\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mget_model(),\n\u001b[1;32m    385\u001b[0m         inputs\u001b[39m=\u001b[39;49mx_batch,\n\u001b[1;32m    386\u001b[0m         targets\u001b[39m=\u001b[39;49my_batch,\n\u001b[1;32m    387\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplain_func_kwargs,\n\u001b[1;32m    388\u001b[0m     )\n\u001b[1;32m    390\u001b[0m \u001b[39m# Expand attributions to input dimensionality.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m a_batch \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mexpand_attribution_channel(a_batch, x_batch)\n",
      "File \u001b[0;32m~/Documents/Thesis/dianna-exploration/relevance_maps_properties/metrics/utils.py:20\u001b[0m, in \u001b[0;36mSHAP_postprocess\u001b[0;34m(label, *args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mSHAP_postprocess\u001b[39m(label, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 20\u001b[0m     shapley_values, segments_slic \u001b[39m=\u001b[39m dianna\u001b[39m.\u001b[39;49mexplain_image(method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mKernelSHAP\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     21\u001b[0m     saliences \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(_fill_segmentation(shapley_values[label][\u001b[39m0\u001b[39m], segments_slic))\n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(saliences)[np\u001b[39m.\u001b[39mnewaxis, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, np\u001b[39m.\u001b[39mnewaxis]\n",
      "File \u001b[0;32m~/Documents/Thesis/dianna-exploration/venv/lib/python3.9/site-packages/dianna/__init__.py:78\u001b[0m, in \u001b[0;36mexplain_image\u001b[0;34m(model_or_function, input_data, method, labels, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m explainer \u001b[39m=\u001b[39m _get_explainer(method, kwargs, modality\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mImage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m explain_image_kwargs \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mget_kwargs_applicable_to_function(explainer\u001b[39m.\u001b[39mexplain, kwargs)\n\u001b[0;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m explainer\u001b[39m.\u001b[39;49mexplain(model_or_function, input_data, labels, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mexplain_image_kwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: explain() got multiple values for argument 'model'"
     ]
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.83s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.85s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.81s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.84s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.74s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.002556783560096721]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
    }
   ],
   "source": [
    "# Average Sensitivity: KernelSHAP\n",
    "quantus.AvgSensitivity(\n",
    "    nr_samples=5,\n",
    "    perturb_func=quantus.perturb_func.gaussian_noise,\n",
    ")(model=k_model, \n",
    "   x_batch=x_batch,\n",
    "   y_batch=y_batch,\n",
    "   a_batch=None,   \n",
    "   explain_func=SHAP_postprocess, \n",
    "   explain_func_kwargs={'model_or_function': onnx_model_path,\n",
    "                        \"input_data\": test_sample,\n",
    "                        \"labels\": range(1),\n",
<<<<<<< HEAD
    "                        \"label\": 0\n",
=======
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
    "                       }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 51,
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Avg Sensitivity metric is likely to be sensitive to the choice of amount of noise added 'lower_bound' and 'upper_bound', the number of samples iterated over 'nr_samples', the function to perturb the input 'perturb_func', the similarity metric 'similarity_func' as well as norm calculations on the numerator and denominator of the sensitivity equation i.e., 'norm_numerator' and 'norm_denominator'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Yeh, Chih-Kuan, et al. 'On the (in) fidelity and sensitivity for explanations.' arXiv preprint arXiv:1901.09392 (2019).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 5000/5000 [00:06<00:00, 714.38it/s]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 711.35it/s]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 681.76it/s]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 652.24it/s]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 692.74it/s]\n",
      "100%|██████████| 5000/5000 [00:06<00:00, 793.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
=======
      "100%|██████████| 5000/5000 [00:07<00:00, 704.54it/s]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 631.43it/s]\n",
      "100%|██████████| 5000/5000 [00:08<00:00, 594.72it/s]\n",
      "100%|██████████| 5000/5000 [00:08<00:00, 620.82it/s]\n",
      " 95%|█████████▌| 4774/5000 [00:07<00:00, 683.47it/s]"
     ]
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
    }
   ],
   "source": [
    "# Average Sensitivity: LIME\n",
    "quantus.AvgSensitivity(\n",
    "    nr_samples=5,\n",
    "    perturb_func=quantus.perturb_func.gaussian_noise,\n",
    ")(model=k_model, \n",
    "   x_batch=x_batch,\n",
    "   y_batch=y_batch,\n",
    "   a_batch=None,   \n",
    "   explain_func=LIME_postprocess, \n",
    "   explain_func_kwargs={'model_or_function': onnx_model_path,\n",
    "                        \"input_data\": test_sample,\n",
    "                        \"labels\": range(1),\n",
    "                       }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Avg Sensitivity metric is likely to be sensitive to the choice of amount of noise added 'lower_bound' and 'upper_bound', the number of samples iterated over 'nr_samples', the function to perturb the input 'perturb_func', the similarity metric 'similarity_func' as well as norm calculations on the numerator and denominator of the sensitivity equation i.e., 'norm_numerator' and 'norm_denominator'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Yeh, Chih-Kuan, et al. 'On the (in) fidelity and sensitivity for explanations.' arXiv preprint arXiv:1901.09392 (2019).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Rise parameter p_keep was automatically determined at 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 10/10 [00:00<00:00, 33.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rise parameter p_keep was automatically determined at 0.30000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 10/10 [00:00<00:00, 31.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rise parameter p_keep was automatically determined at 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 10/10 [00:00<00:00, 33.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rise parameter p_keep was automatically determined at 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 10/10 [00:00<00:00, 31.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rise parameter p_keep was automatically determined at 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 10/10 [00:00<00:00, 29.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rise parameter p_keep was automatically determined at 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 10/10 [00:00<00:00, 32.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08639039471745491]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "outputs": [],
>>>>>>> d7ab830ffd0135166d0145861d004ac5ee8f46da
   "source": [
    "# Average Sensitivity: RISE\n",
    "quantus.AvgSensitivity(\n",
    "    nr_samples=5,\n",
    "    perturb_func=quantus.perturb_func.gaussian_noise,\n",
    ")(model=k_model, \n",
    "   x_batch=x_batch,\n",
    "   y_batch=y_batch,\n",
    "   a_batch=None,   \n",
    "   explain_func=dianna.explain_image, \n",
    "   explain_func_kwargs={'model_or_function': onnx_model_path,\n",
    "                        \"input_data\": test_sample,\n",
    "                        \"method\": \"RISE\",\n",
    "                        \"labels\": range(1),\n",
    "                       }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
