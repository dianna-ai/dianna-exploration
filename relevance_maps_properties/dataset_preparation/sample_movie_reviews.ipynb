{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvocab\u001b[39;00m \u001b[39mimport\u001b[39;00m Vectors\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m get_tokenizer\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ..models import Model\n",
    "\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from dianna.utils.tokenizers import SpacyTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model from disk\n",
    "loaded_model = torch.load('/home/willem/Documents/Thesis/dianna-exploration/relevance_maps_properties/models/movie_review_model.pytorch', map_location=torch.device('cpu'))\n",
    "\n",
    "# store as ONNX, needs example inpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m         pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward(tokens))\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     33\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mhstack((pred, \u001b[39m1\u001b[39m\u001b[39m-\u001b[39mpred))\n\u001b[0;32m---> 35\u001b[0m predict_sentiment \u001b[39m=\u001b[39m Predictor(loaded_model, tokenizer\u001b[39m.\u001b[39mtokenize, vocab, device, max_filter_size\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(filter_sizes))\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membedder\u001b[39m(tokens):\n\u001b[1;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tokens, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loaded_model' is not defined"
     ]
    }
   ],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model, tokenizer, vocab, device, max_filter_size=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        self.max_filter_size = max_filter_size\n",
    "        self.classes = ['negative', 'positive']\n",
    "\n",
    "    def _preprocess_batch(self, sentences):\n",
    "        batch = []\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer(sentence)\n",
    "            # pad if needed\n",
    "            if self.max_filter_size is not None:\n",
    "                tokens = pad(tokens, self.max_filter_size)\n",
    "            # numericalize\n",
    "            tokens = [self.vocab.stoi[token] if token in self.vocab.stoi else self.vocab.stoi['<unk>'] for token in tokens]\n",
    "            batch.append(tokens)\n",
    "        return batch\n",
    "            \n",
    "    def __call__(self, sentences):\n",
    "        # get numerical tokens\n",
    "        # tokens = self.tokenizer(sentence)\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = self.tokenizer(sentences)\n",
    "        batch = self._preprocess_batch(sentences)\n",
    "        # move to device and add required batch axis\n",
    "        tokens = torch.tensor(batch).to(self.device)\n",
    "        # feed to model\n",
    "        pred = torch.sigmoid(self.model.forward(tokens)).detach().numpy()\n",
    "        \n",
    "        return np.hstack((pred, 1-pred))\n",
    "    \n",
    "predict_sentiment = Predictor(loaded_model, tokenizer.tokenize, vocab, device, max_filter_size=max(filter_sizes))\n",
    "\n",
    "def embedder(tokens):\n",
    "    if isinstance(tokens, (np.ndarray, np.generic)):\n",
    "        tokens = tokens.tolist()\n",
    "    # pad if needed\n",
    "    if max_filter_size is not None:\n",
    "        tokens = pad(tokens, max_filter_size)\n",
    "    # numericalize  \n",
    "    tokens = [vocab.stoi[token] if token in vocab.stoi else vocab.stoi['<unk>'] for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict_sentiment \u001b[39m=\u001b[39m Predictor(loaded_model, tokenizer\u001b[39m.\u001b[39mtokenize, vocab, device, max_filter_size\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(filter_sizes))\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membedder\u001b[39m(tokens):\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tokens, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "predict_sentiment = Predictor(loaded_model, tokenizer.tokenize, vocab, device, max_filter_size=max(filter_sizes))\n",
    "\n",
    "def embedder(tokens):\n",
    "    if isinstance(tokens, (np.ndarray, np.generic)):\n",
    "        tokens = tokens.tolist()\n",
    "    # pad if needed\n",
    "    if max_filter_size is not None:\n",
    "        tokens = pad(tokens, max_filter_size)\n",
    "    # numericalize  \n",
    "    tokens = [vocab.stoi[token] if token in vocab.stoi else vocab.stoi['<unk>'] for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# init a model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m output_dim \u001b[39m=\u001b[39m output_dim\n\u001b[1;32m      3\u001b[0m dropout \u001b[39m=\u001b[39m dropout\n\u001b[1;32m      4\u001b[0m n_filters \u001b[39m=\u001b[39m n_filters\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_dim' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences:\n\u001b[0;32m---> 13\u001b[0m     res\u001b[39m.\u001b[39mappend(predict_sentiment([sentence]))\n\u001b[1;32m     14\u001b[0m res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(res)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = pd.read_csv('/home/willem/Documents/Thesis/dianna-exploration/relevance_maps_properties/data/test .tsv', delimiter='\\t')['sentence']\n",
    "nsamples = 100\n",
    "classes = ['negative', 'positive']\n",
    "\n",
    "# for n, sentence in enumerate(sentences):\n",
    "#     if n == nmax:\n",
    "#         break\n",
    "#     output_numerical = predict_sentiment([sentence])\n",
    "#     print(f\"\\\"{sentence}\\\" - {output_numerical}\")\n",
    "\n",
    "res = []\n",
    "for sentence in sentences:\n",
    "    res.append(predict_sentiment([sentence]))\n",
    "res = np.array(res).reshape(-1, 2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
